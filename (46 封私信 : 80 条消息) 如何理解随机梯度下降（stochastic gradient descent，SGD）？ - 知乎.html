<!doctype html>
<html lang="zh" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">如何理解随机梯度下降（stochastic gradient descent，SGD）？ - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="description" property="og:description" content="它的优缺点在哪？为什么效率比较高？有什么理论支持吗？有实例分析证明收敛性吗？据说在训练 ML、NN 时用…"/><meta data-react-helmet="true" name="keywords" content="数学,机器学习,优化,凸优化,神经网络"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.a53ae37b.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.a53ae37b.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.bbce8f18.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.cbade8f9.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.8f6c52aa.png" sizes="60x60"/><link crossorigin="" rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/heifetz/favicon.ico"/><link crossorigin="" rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/heifetz/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/main.app.216a26f4.da22a0b66acb98396332.css" crossorigin="" rel="stylesheet"/><link crossorigin="" href="https://static.zhihu.com/heifetz/main.question-routes.216a26f4.3f496e7708e56829f55f.css" rel="stylesheet"/><script nonce="fef46db5-af50-4cd0-89e4-9a6bb015b871">!function(){"use strict";!function(e,n){var r,t,a,c,o,i=[];function s(e){return function(){i.push([e,arguments])}}function u(e){var n=e;if(!(e instanceof Error||e instanceof ErrorEvent||e instanceof DOMError||e instanceof DOMException)){var r=e.message||e.reason;n=Error(r)}Raven.captureException(n)}n.Raven={captureException:s("captureException"),captureMessage:s("captureMessage"),captureBreadcrumb:s("captureBreadcrumb")},n.addEventListener("unhandledrejection",u),n.addEventListener("error",u,!0),r=e.src,t=e,a=function(){i.forEach(function(e){var n;(n=Raven)[e[0]].apply(n,e[1])}),n.removeEventListener("unhandledrejection",u),n.removeEventListener("error",u,!0)},c=document.head||document.getElementsByTagName("head")[0],(o=document.createElement("script")).crossOrigin=t.crossOrigin,o.dataset.sentryConfig=t["data-sentry-config"],o.onload=a,o.src=r,c.appendChild(o)}({"defer":true,"crossOrigin":"anonymous","src":"https://unpkg.zhimg.com/@cfe/sentry-script@1.3.0/dist/init.js","data-sentry-config":"{\"dsn\":\"https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224\",\"sampleRate\":0.1,\"release\":\"2671-9af6d8ff\",\"ignoreErrorNames\":[\"NetworkError\",\"SecurityError\"],\"ignoreErrorsPreset\":\"ReactApp\"}"},window)}();
</script><script nonce="fef46db5-af50-4cd0-89e4-9a6bb015b871">if (window.requestAnimationFrame) {    window.requestAnimationFrame(function() {      window.FIRST_ANIMATION_FRAME = Date.now();    });  }</script></head><body><div id="root"><div><div class="LoadingBar"></div><div><header role="banner" class="Sticky AppHeader" data-za-module="TopNavBar"><style data-emotion-css="qqgmyv">.css-qqgmyv{width:auto;max-width:1156px;min-width:1000px;padding-left:16px;padding-right:30px;}.css-qqgmyv .AppHeader-userInfo{margin-left:30px;width:auto;}.css-qqgmyv .AppHeader-TabsLink.is-active,.css-qqgmyv .AppHeader-TabsLink:hover{color:#121212;}</style><div class="AppHeader-inner css-qqgmyv"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 64 30" fill="#0066FF" width="64" height="30"><path d="M29.05 4.582H16.733V25.94h3.018l.403 2.572 4.081-2.572h4.815V4.582zm-5.207 18.69l-2.396 1.509-.235-1.508h-1.724V7.233h6.78v16.04h-2.425zM14.46 14.191H9.982c0-.471.033-.954.039-1.458v-5.5h5.106V5.935a1.352 1.352 0 0 0-.404-.957 1.378 1.378 0 0 0-.968-.396H5.783c.028-.088.056-.177.084-.255.274-.82 1.153-3.326 1.153-3.326a4.262 4.262 0 0 0-2.413.698c-.57.4-.912.682-1.371 1.946-.532 1.453-.997 2.856-1.31 3.693C1.444 8.674.28 11.025.28 11.025a5.85 5.85 0 0 0 2.52-.61c1.119-.593 1.679-1.502 2.054-2.883l.09-.3h2.334v5.5c0 .5-.045.982-.073 1.46h-4.12c-.71 0-1.39.278-1.893.775a2.638 2.638 0 0 0-.783 1.874h6.527a17.717 17.717 0 0 1-.778 3.649 16.796 16.796 0 0 1-3.012 5.273A33.104 33.104 0 0 1 0 28.74s3.13 1.175 5.425-.954c1.388-1.292 2.631-3.814 3.23-5.727a28.09 28.09 0 0 0 1.12-5.229h5.967v-1.37a1.254 1.254 0 0 0-.373-.899 1.279 1.279 0 0 0-.909-.37z"></path><path d="M11.27 19.675l-2.312 1.491 5.038 7.458a6.905 6.905 0 0 0 .672-2.218 3.15 3.15 0 0 0-.28-2.168l-3.118-4.563zM51.449 15.195V5.842c4.181-.205 7.988-.405 9.438-.483l.851-.05c.387-.399.885-2.395.689-3.021-.073-.25-.213-.666-.638-.555a33.279 33.279 0 0 1-4.277.727c-2.766.321-3.97.404-7.804.682-6.718.487-12.709.72-12.709.72a2.518 2.518 0 0 0 .788 1.834 2.567 2.567 0 0 0 1.883.706c2.278-.095 5.598-.25 8.996-.41v9.203h-12.78c0 .703.281 1.377.783 1.874a2.69 2.69 0 0 0 1.892.777h10.105v7.075c0 .887-.464 1.192-1.231 1.214h-3.92a4.15 4.15 0 0 0 .837 1.544 4.2 4.2 0 0 0 1.403 1.067 6.215 6.215 0 0 0 2.71.277c1.36-.066 2.967-.826 2.967-3.57v-7.607h11.28c.342 0 .67-.135.91-.374.242-.239.378-.563.378-.902v-1.375H51.449z"></path><path d="M42.614 8.873a2.304 2.304 0 0 0-1.508-.926 2.334 2.334 0 0 0-1.727.405l-.376.272 4.255 5.85 2.24-1.62-2.884-3.98zM57.35 8.68l-3.125 4.097 2.24 1.663 4.517-5.927-.375-.277a2.32 2.32 0 0 0-1.722-.452 2.327 2.327 0 0 0-1.536.896z"></path></svg></a><style data-emotion-css="g0ay3v">.css-g0ay3v{margin-left:25px;margin-right:15px;}.css-g0ay3v .AppHeader-Tab{padding-left:15px;padding-right:15px;}.css-g0ay3v .Tabs-link.is-active::after{height:4px;}</style><ul role="navigation" class="Tabs AppHeader-Tabs css-g0ay3v"><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a tabindex="0" class="Tabs-link AppHeader-TabsLink" href="//www.zhihu.com/" data-za-not-track-link="true">首页</a></li><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a tabindex="0" class="Tabs-link AppHeader-TabsLink" href="//www.zhihu.com/xen/vip-web" data-za-not-track-link="true">会员</a></li><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a tabindex="0" class="Tabs-link AppHeader-TabsLink" href="//www.zhihu.com/explore" data-za-not-track-link="true">发现</a></li><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a tabindex="0" class="Tabs-link AppHeader-TabsLink" href="//www.zhihu.com/question/waiting" data-za-not-track-link="true">等你来答</a></li></ul><style data-emotion-css="1acwmmj">.css-1acwmmj{box-sizing:border-box;margin:0;min-width:0;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><div class="css-1acwmmj"><style data-emotion-css="10fy1q8">.css-10fy1q8{max-width:482px;}.css-10fy1q8 .SearchBar-input{border-radius:999px;padding-left:16px;}.css-10fy1q8 .SearchBar-askButton{border-radius:999px;width:70px;margin-left:12px;}.css-10fy1q8 .SearchBar-searchButton{border-bottom-right-radius:999px;border-top-right-radius:999px;}</style><div class="SearchBar AppHeader-SearchBar css-10fy1q8" role="search" data-za-module="PresetWordItem"><form class="SearchBar-tool"><div><div class="Popover"><label class="SearchBar-input Input-wrapper Input-wrapper--grey"><input type="text" maxLength="100" value="" autoComplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="null--1" id="null-toggle" aria-haspopup="true" aria-owns="null-content" class="Input" placeholder=""/><button aria-label="搜索" type="button" class="Button SearchBar-searchButton Button--primary"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Search SearchBar-searchIcon" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><path d="M17.068 15.58a8.377 8.377 0 0 0 1.774-5.159 8.421 8.421 0 1 0-8.42 8.421 8.38 8.38 0 0 0 5.158-1.774l3.879 3.88c.957.573 2.131-.464 1.488-1.49l-3.879-3.878zm-6.647 1.157a6.323 6.323 0 0 1-6.316-6.316 6.323 6.323 0 0 1 6.316-6.316 6.323 6.323 0 0 1 6.316 6.316 6.323 6.323 0 0 1-6.316 6.316z" fill-rule="evenodd"></path></svg></span></button></label></div></div></form><button type="button" class="Button SearchBar-askButton Button--primary Button--blue">提问</button></div></div><div class="AppHeader-userInfo"><style data-emotion-css="79elbk">.css-79elbk{position:relative;}</style><button type="button" class="Button AppHeader-notifications css-79elbk Button--plain"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Bell" fill="currentColor" viewBox="0 0 24 24" width="22" height="22"><path d="M4.523 15.076l.804-6.757a6.753 6.753 0 0 1 4.945-5.7 1.823 1.823 0 0 1 3.623 0 6.753 6.753 0 0 1 4.945 5.7l.804 6.757a2.293 2.293 0 0 0 1.712 2.108 1.093 1.093 0 0 1-.297 2.15H3.108a1.093 1.093 0 0 1-.297-2.15 2.293 2.293 0 0 0 1.712-2.108zM12.083 23a2.758 2.758 0 0 1-2.753-2.509.229.229 0 0 1 .232-.24h5.043a.229.229 0 0 1 .232.24 2.759 2.759 0 0 1-2.753 2.51z"></path></svg></span></button><button type="button" class="Button AppHeader-messages css-79elbk Button--plain"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comments" fill="currentColor" viewBox="0 0 24 24" width="22" height="22"><path d="M11 2c5.571 0 9 4.335 9 8 0 6-6.475 9.764-11.481 8.022-.315-.07-.379-.124-.78.078-1.455.54-2.413.921-3.525 1.122-.483.087-.916-.25-.588-.581 0 0 .677-.417.842-1.904.064-.351-.14-.879-.454-1.171A8.833 8.833 0 0 1 2 10c0-3.87 3.394-8 9-8zm10.14 9.628c.758.988.86 2.009.86 3.15 0 1.195-.619 3.11-1.368 3.938-.209.23-.354.467-.308.722.12 1.073.614 1.501.614 1.501.237.239-.188.562-.537.5-.803-.146-1.495-.42-2.546-.811-.29-.146-.336-.106-.563-.057-2.043.711-4.398.475-6.083-.927 5.965-.524 8.727-3.03 9.93-8.016z" fill-rule="evenodd"></path></svg></span></button><div class="AppHeader-profile"><button type="button" class="Button AppHeader-profileEntry Button--plain"><img class="Avatar AppHeader-profileAvatar AppHeader-profileAvatar--old" width="30" height="30" src="https://pic1.zhimg.com/da8e974dc_is.jpg" srcSet="https://pic1.zhimg.com/da8e974dc_im.jpg 2x" alt="点击打开undefined的主页"/></button></div></div></div><div></div></header></div><main role="main" class="App-main"><div class="QuestionPage" itemscope="" itemType="http://schema.org/Question"><meta itemProp="name" content="如何理解随机梯度下降（stochastic gradient descent，SGD）？"/><meta itemProp="url" content="https://www.zhihu.com/question/264189719"/><meta itemProp="keywords" content="数学,机器学习,优化,凸优化,神经网络"/><meta itemProp="answerCount" content="16"/><meta itemProp="commentCount" content="0"/><meta itemProp="dateCreated" content="2017-12-16T19:06:22.000Z"/><meta itemProp="dateModified" content="2019-08-10T05:18:02.000Z"/><meta itemProp="zhihu:visitsCount"/><meta itemProp="zhihu:followerCount" content="586"/><div data-zop-question="{&quot;title&quot;:&quot;如何理解随机梯度下降（stochastic gradient descent，SGD）？&quot;,&quot;topics&quot;:[{&quot;name&quot;:&quot;数学&quot;,&quot;id&quot;:&quot;19554091&quot;},{&quot;name&quot;:&quot;机器学习&quot;,&quot;id&quot;:&quot;19559450&quot;},{&quot;name&quot;:&quot;优化&quot;,&quot;id&quot;:&quot;19570512&quot;},{&quot;name&quot;:&quot;凸优化&quot;,&quot;id&quot;:&quot;19602355&quot;},{&quot;name&quot;:&quot;神经网络&quot;,&quot;id&quot;:&quot;19607065&quot;}],&quot;id&quot;:264189719,&quot;isEditable&quot;:false}"><div><div class="QuestionHeader"><div class="QuestionHeader-content"><div class="QuestionHeader-main"><div class="QuestionHeader-tags"><div class="QuestionHeader-topics"><div class="Tag QuestionTopic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19554091" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">数学</div></div></a></span></div><div class="Tag QuestionTopic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19559450" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">机器学习</div></div></a></span></div><div class="Tag QuestionTopic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19570512" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">优化</div></div></a></span></div><div class="Tag QuestionTopic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19602355" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">凸优化</div></div></a></span></div><div class="Tag QuestionTopic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19607065" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">神经网络</div></div></a></span></div></div></div><h1 class="QuestionHeader-title">如何理解随机梯度下降（stochastic gradient descent，SGD）？</h1><div><style data-emotion-css="eew49z">.css-eew49z{min-height:10px;}</style><div class="css-eew49z"><div class="QuestionRichText QuestionRichText--expandable QuestionRichText--collapsed"><div><span>它的优缺点在哪？为什么效率比较高？有什么理论支持吗？有实例分析证明收敛性吗？据说在训练 ML、NN 时用的最多，是真的吗？刚接触优化理论，谢谢大家分享…</span><button type="button" class="Button QuestionRichText-more Button--plain">显示全部 <span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--ArrowDown" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div><div class="QuestionHeader-side"><div class="QuestionHeader-follow-status"><div class="QuestionFollowStatus"><div class="NumberBoard QuestionFollowStatus-counts NumberBoard--divider"><button type="button" class="Button NumberBoard-item Button--plain"><div class="NumberBoard-itemInner"><div class="NumberBoard-itemName">关注者</div><strong class="NumberBoard-itemValue" title="586">586</strong></div></button><div class="NumberBoard-item"><div class="NumberBoard-itemInner"><div class="NumberBoard-itemName">被浏览</div><strong class="NumberBoard-itemValue" title="187436">187,436</strong></div></div></div></div></div></div></div><div class="QuestionHeader-footer"><div class="QuestionHeader-footer-inner"><div class="QuestionHeader-main QuestionHeader-footer-main"><div class="QuestionButtonGroup"><button type="button" class="Button FollowButton Button--primary Button--blue">关注问题</button><button type="button" class="Button Button--blue"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Edit QuestionButton-icon" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M4.076 16.966a4.19 4.19 0 0 1 1.05-1.76l8.568-8.569a.524.524 0 0 1 .741 0l2.928 2.927a.524.524 0 0 1 0 .74l-8.568 8.57c-.49.49-1.096.852-1.761 1.051l-3.528 1.058a.394.394 0 0 1-.49-.488l1.06-3.53zM20.558 4.83c.59.59.59 1.546 0 2.136l-1.693 1.692a.503.503 0 0 1-.712 0l-2.812-2.812a.504.504 0 0 1 0-.712l1.693-1.693a1.51 1.51 0 0 1 2.135 0l1.389 1.389z"></path></svg></span>写回答</button></div><div class="QuestionHeaderActions"><button style="margin-right:16px" type="button" class="Button Button--grey Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Invite Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M4 10V8a1 1 0 1 1 2 0v2h2a1 1 0 0 1 0 2H6v2a1 1 0 0 1-2 0v-2H2a1 1 0 0 1 0-2h2zm10.455 2c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm-7 6c0-2.66 4.845-4 7.272-4C17.155 14 22 15.34 22 18v1.375c0 .345-.28.625-.625.625H8.08a.625.625 0 0 1-.625-.625V18z" fill-rule="evenodd"></path></svg></span>邀请回答</button><div class="GoodQuestionAction"><button type="button" class="Button GoodQuestionAction-commonBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Like Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>好问题 18</button></div><div class="QuestionHeader-Comment"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button></div><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><div class="Popover"><button aria-label="更多" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content" type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div><div class="QuestionHeader-actions"></div></div></div></div></div></div><div><div class="Sticky"></div></div></div><div class="Question-main"><div class="Question-mainColumn"><div><div class="QuestionAnswers-statusWrapper"></div><div id="QuestionAnswers-answers" class="QuestionAnswers-answers" data-zop-feedlistmap="0,0,1,0"><div class="Card AnswersNavWrapper"><div class="ListShortcut"><div class="List"><div class="List-header"><h4 class="List-headerText"><span>16<!-- --> 个回答</span></h4><div class="List-headerOptions"><div class="Popover"><button role="combobox" aria-expanded="false" id="null-toggle" aria-haspopup="true" aria-owns="null-content" type="button" class="Button InputLike InputButton Select-button Select-plainButton Button--plain">默认排序<svg class="Zi Zi--Select Select-arrow" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></button></div></div></div><div><div class=""><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="0" data-zop="{&quot;authorName&quot;:&quot;Evan&quot;,&quot;itemId&quot;:291167114,&quot;title&quot;:&quot;如何理解随机梯度下降（stochastic gradient descent，SGD）？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="291167114" itemProp="acceptedAnswer" itemType="http://schema.org/Answer" itemscope=""><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemProp="author" itemscope="" itemType="http://schema.org/Person"><div class="AuthorInfo"><meta itemProp="name" content="Evan"/><meta itemProp="image" content="https://pic1.zhimg.com/v2-f7135f3f439afc5759dec917272c10d7_l.jpg?source=1940ef5c"/><meta itemProp="url" content="https://www.zhihu.com/people/yzhihao"/><meta itemProp="zhihu:followerCount" content="10140"/><span class="UserLink AuthorInfo-avatarWrapper"><a target="_blank" class="UserLink-link" data-za-detail-view-element_name="User" href="//www.zhihu.com/people/yzhihao"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="https://pic1.zhimg.com/v2-f7135f3f439afc5759dec917272c10d7_xs.jpg?source=1940ef5c" srcSet="https://pic1.zhimg.com/v2-f7135f3f439afc5759dec917272c10d7_l.jpg?source=1940ef5c 2x" alt="Evan"/></a></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><a target="_blank" class="UserLink-link" data-za-detail-view-element_name="User" href="//www.zhihu.com/people/yzhihao">Evan</a><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">欢迎关注我的专栏</div></div></div></div></div></div><style data-emotion-css="h5al4j">.css-h5al4j{box-sizing:border-box;margin:0;min-width:0;color:#8590A6;font-size:14px;margin-top:10px;margin-bottom:-4px;}</style><div class="css-h5al4j"><span><span class="Voters"><button type="button" class="Button Button--plain">572 人<!-- -->赞同了该回答</button></span></span></div></div><meta itemProp="image"/><meta itemProp="upvoteCount" content="572"/><meta itemProp="url" content="https://www.zhihu.com/question/264189719/answer/291167114"/><meta itemProp="dateCreated" content="2018-01-07T13:14:46.000Z"/><meta itemProp="dateModified" content="2018-01-07T14:30:13.000Z"/><meta itemProp="commentCount" content="0"/><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemProp="text"><p>因为觉得之前的回答都不是很直接和全面，在下回答一波：</p><p class="ztext-empty-paragraph"><br/></p><p><b>理解随机梯度下降，首先要知道梯度下降法，故先介绍梯度下降法：</b></p><h2>梯度下降法</h2><p>大多数机器学习或者深度学习算法都涉及某种形式的优化。 优化指的是改变 <img src="https://www.zhihu.com/equation?tex=x+" alt="x " eeimg="1"/> 以最小化或最大化某个函数 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="f(x)" eeimg="1"/> 的任务。 我们通常以最小化 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="f(x)" eeimg="1"/> 指代大多数最优化问题。 最大化可经由最小化算法最小化 <img src="https://www.zhihu.com/equation?tex=-f%28x%29" alt="-f(x)" eeimg="1"/> 来实现。</p><p>我们把要最小化或最大化的函数称为<b>目标函数或准则</b>。 当我们对其进行最小化时，我们也把它称为<b>代价函数、损失函数或误差函数</b>。 </p><p>下面，我们假设一个损失函数为 <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29%3D%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%28%7Bh_%5Ctheta%28x%29-y%7D%29%5E2" alt="J(\theta)=\frac{1}{2}\sum_{i=1}^{m}({h_\theta(x)-y})^2" eeimg="1"/> ，其中 <img src="https://www.zhihu.com/equation?tex=h_%CE%B8%28x%29%3D%CE%B8_0%2B%CE%B8_1x_1%2B%CE%B8_%7B2%7Dx_2....%2B%CE%B8_%7Bn%7Dx_n" alt="h_θ(x)=θ_0+θ_1x_1+θ_{2}x_2....+θ_{n}x_n" eeimg="1"/> 然后要使得最小化它。</p><blockquote>注意：这里只是假设，不用知道这个目标函数就是平方损失函数等等，然后肯定有人问既然要最小化它，那求个导数，然后使得导数等于0求出不就好了吗？Emmmm...是的，有这样的解法，可以去了解正规方程组求解。说下这里不讲的原因，主要是那样的方式太难求解，然后在高维的时候，可能不可解，但机器学习或深度学习中，很多都是超高维的，所以也一般不用那种方法。总之，梯度下降是另一种优化的不错方式，比直接求导好很多。</blockquote><p><b>梯度下降：</b>我们知道曲面上方向导数的最大值的方向就代表了梯度的方向，因此我们在做梯度下降的时候，应该是沿着梯度的反方向进行权重的更新，可以有效的找到全局的最优解。这个 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_i" alt="\theta_i" eeimg="1"/> 的更新过程可以描述为</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-b8845b3f88cb39bc2ecf2eabefa9d7b4_hd.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="480" data-rawheight="313" class="origin_image zh-lightbox-thumb" width="480" data-original="https://pic2.zhimg.com/v2-b8845b3f88cb39bc2ecf2eabefa9d7b4_r.jpg?source=1940ef5c"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;480&#39; height=&#39;313&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="480" data-rawheight="313" class="origin_image zh-lightbox-thumb lazy" width="480" data-original="https://pic2.zhimg.com/v2-b8845b3f88cb39bc2ecf2eabefa9d7b4_r.jpg?source=1940ef5c" data-actualsrc="https://pic1.zhimg.com/50/v2-b8845b3f88cb39bc2ecf2eabefa9d7b4_hd.jpg?source=1940ef5c"/></figure><p>[a表示的是步长或者说是学习率（learning rate）]</p><p>好了，怎么理解？在直观上，我们可以这样理解，看下图，一开始的时候我们随机站在一个点，把他看成一座山，每一步，我们都以下降最多的路线来下山，那么，在这个过程中我们到达山底（最优点）是最快的，而上面的a，它决定了我们“向下山走”时每一步的大小，过小的话收敛太慢，过大的话可能错过最小值，扯到蛋...）。这是一种很自然的算法，每一步总是寻找使J下降最“陡”的方向（就像找最快下山的路一样）。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/50/v2-77ab0b5fab734c998db19771ed81a0dc_hd.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="641" data-rawheight="312" class="origin_image zh-lightbox-thumb" width="641" data-original="https://pic1.zhimg.com/v2-77ab0b5fab734c998db19771ed81a0dc_r.jpg?source=1940ef5c"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;641&#39; height=&#39;312&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="641" data-rawheight="312" class="origin_image zh-lightbox-thumb lazy" width="641" data-original="https://pic1.zhimg.com/v2-77ab0b5fab734c998db19771ed81a0dc_r.jpg?source=1940ef5c" data-actualsrc="https://pic2.zhimg.com/50/v2-77ab0b5fab734c998db19771ed81a0dc_hd.jpg?source=1940ef5c"/></figure><p>当然了，我们直观上理解了之后，接下来肯定是从数学的角度，我们可以这样想，先想在低维的时候，比如二维，我们要找到最小值，其实可以是这样的方法，具体化到1元函数中时，梯度方向首先是沿着曲线的切线的，然后取切线向上增长的方向为梯度方向，2元或者多元函数中，梯度向量为函数值f对每个变量的导数，该向量的方向就是梯度的方向，当然向量的大小也就是梯度的大小。现在假设我们要求函数的最值，采用梯度下降法，结合如图所示：</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/50/v2-982a9e8f26297deff656990e049383b8_hd.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="1022" data-rawheight="774" class="origin_image zh-lightbox-thumb" width="1022" data-original="https://pic1.zhimg.com/v2-982a9e8f26297deff656990e049383b8_r.jpg?source=1940ef5c"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1022&#39; height=&#39;774&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1022" data-rawheight="774" class="origin_image zh-lightbox-thumb lazy" width="1022" data-original="https://pic1.zhimg.com/v2-982a9e8f26297deff656990e049383b8_r.jpg?source=1940ef5c" data-actualsrc="https://pic3.zhimg.com/50/v2-982a9e8f26297deff656990e049383b8_hd.jpg?source=1940ef5c"/></figure><blockquote>如图所示，我们假设函数是 <img src="https://www.zhihu.com/equation?tex=y%3Dx%5E2%2B1" alt="y=x^2+1" eeimg="1"/> ,那么如何使得这个函数达到最小值呢，简单的理解，就是对x求导，得到 <img src="https://www.zhihu.com/equation?tex=y%E2%80%98%3D%5Cfrac%7B1%7D%7B2%7Dx" alt="y‘=\frac{1}{2}x" eeimg="1"/> ，然后用梯度下降的方式，如果初始值是（0的左边）负值,那么这是导数也是负值，用梯度下降的公式，使得x更加的靠近0,如果是正值的时候同理。<b>注意：这里的梯度也就是一元函数的导数，高维的可以直接类推之</b></blockquote><p class="ztext-empty-paragraph"><br/></p><p class="ztext-empty-paragraph"><br/></p><p><b>然后是优缺点，这里比较对象是批量梯度和mini-batch梯度下降，先看下他们三者：</b></p><ul><li>批量梯度下降：在每次更新时用所有样本，要留意，在梯度下降中，对于 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_i" alt="\theta_i" eeimg="1"/> 的更新，所有的样本都有贡献，也就是参与调整 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta" eeimg="1"/> .其计算得到的是一个标准梯度，<b>对于最优化问题，凸问题，</b>也肯定可以达到一个全局最优。因而理论上来说一次更新的幅度是比较大的。如果样本不多的情况下，当然是这样收敛的速度会更快啦。但是很多时候，样本很多，更新一次要很久，这样的方法就不合适啦。下图是其更新公式</li></ul><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-5809743fd06c4ff804753d29e4b83935_hd.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="620" data-rawheight="148" class="origin_image zh-lightbox-thumb" width="620" data-original="https://pic1.zhimg.com/v2-5809743fd06c4ff804753d29e4b83935_r.jpg?source=1940ef5c"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;620&#39; height=&#39;148&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="620" data-rawheight="148" class="origin_image zh-lightbox-thumb lazy" width="620" data-original="https://pic1.zhimg.com/v2-5809743fd06c4ff804753d29e4b83935_r.jpg?source=1940ef5c" data-actualsrc="https://pic1.zhimg.com/50/v2-5809743fd06c4ff804753d29e4b83935_hd.jpg?source=1940ef5c"/></figure><ul><li>随机梯度下降：在每次更新时用1个样本，可以看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整<i>θ</i>，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，<b>对于最优化问题，凸问题，</b>虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。但是相比于批量梯度，这样的方法更快，更快收敛，虽然不是全局最优，但很多时候是我们可以接受的，所以这个方法用的也比上面的多。下图是其更新公式：</li></ul><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/50/v2-b3f14a09ad27df9c66a3af208060f5d7_hd.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="616" data-rawheight="230" class="origin_image zh-lightbox-thumb" width="616" data-original="https://pic1.zhimg.com/v2-b3f14a09ad27df9c66a3af208060f5d7_r.jpg?source=1940ef5c"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;616&#39; height=&#39;230&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="616" data-rawheight="230" class="origin_image zh-lightbox-thumb lazy" width="616" data-original="https://pic1.zhimg.com/v2-b3f14a09ad27df9c66a3af208060f5d7_r.jpg?source=1940ef5c" data-actualsrc="https://pic3.zhimg.com/50/v2-b3f14a09ad27df9c66a3af208060f5d7_hd.jpg?source=1940ef5c"/></figure><ul><li>mini-batch梯度下降：在每次更新时用b个样本,其实批量的梯度下降就是一种折中的方法，他用了一些小样本来近似全部的，其本质就是我1个指不定不太准，那我用个30个50个样本那比随机的要准不少了吧，而且批量的话还是非常可以反映样本的一个分布情况的。在深度学习中，这种方法用的是最多的，因为这个方法收敛也不会很慢，收敛的局部最优也是更多的可以接受！</li></ul><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/50/v2-96181aa7bc39a7e5fb54cfdf2c42a7b9_hd.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="373" data-rawheight="292" class="content_image" width="373"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;373&#39; height=&#39;292&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="373" data-rawheight="292" class="content_image lazy" width="373" data-actualsrc="https://pic3.zhimg.com/50/v2-96181aa7bc39a7e5fb54cfdf2c42a7b9_hd.jpg?source=1940ef5c"/></figure><p>                                }</p><p>了解之后，总的来说，随机梯度下降一般来说效率高，收敛到的路线曲折，但一般得到的解是我们能够接受的，<b>在深度学习中，用的比较多的是mini-batch梯度下降。</b></p><p class="ztext-empty-paragraph"><br/></p><p><b>最后是收敛性，能收敛吗？收敛到什么地方？</b></p><p>对于收敛性的问题，知乎上就有这个问题：<a href="https://www.zhihu.com/question/27012077" class="internal">为什么随机梯度下降方法能够收敛？</a>，我比较赞赏<a href="https://www.zhihu.com/question/27012077/answer/122359602" class="internal">李文哲博士的回答</a>（推荐一看），总的来说就是从expected loss用特卡洛（monte carlo）来表示计算，那batch GD, mini-batch GD, SGD都可以看成SGD的范畴。因为大家都是在一个真实的分布中得到的样本，对于分布的拟合都是近似的。那这个时候三种方式的梯度下降就都是可以看成用样本来近似分布的过程，都是可以收敛的！</p><p class="ztext-empty-paragraph"><br/></p><p><b>对于收敛到什么地方：</b></p><p>能到的地方：最小值，极小值，鞍点。这些都是能收敛到的地方，也就是梯度为0的点。</p><p>当然，几乎不存在找到鞍点的可能，除非很碰巧，因为梯度下降是对损失函数每个维度分别求极小值，即分别求 <img src="https://www.zhihu.com/equation?tex=J%28%CE%B8%29" alt="J(θ)" eeimg="1"/> 关于 <img src="https://www.zhihu.com/equation?tex=%CE%B8_1%E3%80%81%CE%B8_2...%CE%B8_n" alt="θ_1、θ_2...θ_n" eeimg="1"/> 极小值。</p><p>然后是最小值和极小值，如果是<b>凸函数</b>，梯度下降会收敛到最小值，因为只有一个极小值，它就是最小值。</p><blockquote>至于什么是凸函数，详见我的专栏文章：<a href="https://zhuanlan.zhihu.com/p/30486793" class="internal">掌握机器学习数学基础之凸优化</a>。</blockquote><p class="ztext-empty-paragraph"><br/></p><p><b>对于理论支持：</b></p><p>Optimization Methods for Large-Scale Machine Learning：这论文之前的问答也看到了，贴下知友的翻译。<a href="https://zhuanlan.zhihu.com/p/28060786" class="internal">为什么我们更宠爱“随机”梯度下降？</a></p><p>ROBUST STOCHASTIC APPROXIMATION APPROACH TO STOCHASTIC PROGRAMMING</p><p>An Introduction to optimization</p><p>以上三个关于优化的文章，一切问题，自然随之而解。值得一看！</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="//www.zhihu.com/question/264189719/answer/291167114"><span data-tooltip="发布于 2018-01-07 21:14">编辑于 2018-01-07</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 572 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 572</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><div class="Popover ContentItem-action"><button aria-label="更多" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content" type="button" class="Button OptionsButton Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div><div class="ModalWrap"><div><div class=""></div><div class="ModalExp-content"><div class="ModalWrap-content"><div class="ModalWrap-title">继续浏览内容</div><div class="ModalWrap-item"><div class="ModalWrap-itemImg"><img src="https://pic4.zhimg.com/80/v2-88158afcff1e7f4b8b00a1ba81171b61_720w.png" alt=""/></div><div class="ModalWrap-itemContent"><div class="ModalWrap-itemTitle">知乎</div><div class="ModalWrap-itemDesc">发现更大的世界</div></div><div class="ModalWrap-itemBtn">打开</div></div><div class="ModalWrap-item"><div class="ModalWrap-itemImg"><img src="https://pic2.zhimg.com/80/v2-da7d9e4b6a7ddba507299bcf5a4d0600_1440w.png" alt=""/></div><div class="ModalWrap-itemContent"><div class="ModalWrap-itemTitle">Safari</div></div><div class="ModalWrap-itemBtn">继续</div></div></div></div></div></div><div><div><div class=""></div><div class="ModalLoading-content"><svg width="30" height="30" viewBox="0 0 66 66" xmlns="http://www.w3.org/2000/svg" class="CircleLoadingBar" aria-hidden="true"><g><circle class="path" fill="none" stroke-width="6" stroke-linecap="round" cx="33" cy="33" r="30"></circle></g></svg></div></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="1" data-zop="{&quot;authorName&quot;:&quot;SofaSofa.io&quot;,&quot;itemId&quot;:279814788,&quot;title&quot;:&quot;如何理解随机梯度下降（stochastic gradient descent，SGD）？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="279814788" itemProp="suggestedAnswer" itemType="http://schema.org/Answer" itemscope=""><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemProp="author" itemscope="" itemType="http://schema.org/Person"><div class="AuthorInfo"><meta itemProp="name" content="SofaSofa.io"/><meta itemProp="image" content="https://pic3.zhimg.com/v2-77984d3daa2c48ac9fc3bbfae5ad31d0_l.jpg?source=1940ef5c"/><meta itemProp="url" content="https://www.zhihu.com/people/SofaSofaIO"/><meta itemProp="zhihu:followerCount" content="2213"/><span class="UserLink AuthorInfo-avatarWrapper"><a target="_blank" class="UserLink-link" data-za-detail-view-element_name="User" href="//www.zhihu.com/people/SofaSofaIO"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="https://pic3.zhimg.com/v2-77984d3daa2c48ac9fc3bbfae5ad31d0_xs.jpg?source=1940ef5c" srcSet="https://pic3.zhimg.com/v2-77984d3daa2c48ac9fc3bbfae5ad31d0_l.jpg?source=1940ef5c 2x" alt="SofaSofa.io"/></a></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><a target="_blank" class="UserLink-link" data-za-detail-view-element_name="User" href="//www.zhihu.com/people/SofaSofaIO">SofaSofa.io</a><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">SofaSofa.io做最好的数据科学社区</div></div></div></div></div></div><style data-emotion-css="h5al4j">.css-h5al4j{box-sizing:border-box;margin:0;min-width:0;color:#8590A6;font-size:14px;margin-top:10px;margin-bottom:-4px;}</style><div class="css-h5al4j"><span><span class="Voters"><button type="button" class="Button Button--plain">34 人<!-- -->赞同了该回答</button></span></span></div></div><meta itemProp="image"/><meta itemProp="upvoteCount" content="34"/><meta itemProp="url" content="https://www.zhihu.com/question/264189719/answer/279814788"/><meta itemProp="dateCreated" content="2017-12-20T07:43:06.000Z"/><meta itemProp="dateModified" content="2017-12-20T07:43:06.000Z"/><meta itemProp="commentCount" content="0"/><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemProp="text"><p>谢谢邀请。</p><p>耐心读完<a href="https://link.zhihu.com/?target=http%3A//sofasofa.io/tutorials/python_gradient_descent/" class=" wrap external" target="_blank" rel="nofollow noreferrer">自己动手用python写梯度下降-SofaSofa</a>，您就能完全理解了（前面是理论，后面是python实现）。</p><p>此外，欢迎关注：<a href="https://link.zhihu.com/?target=http%3A//sofasofa.io/forum_main_popular.php%3Ff%3D1000005" class=" wrap external" target="_blank" rel="nofollow noreferrer">数值计算-热门问题-SofaSofa</a></p><p class="ztext-empty-paragraph"><br/></p><p>谢谢阅读！</p><p><a href="https://link.zhihu.com/?target=http%3A//SofaSofa.io" class=" wrap external" target="_blank" rel="nofollow noreferrer">SofaSofa.io</a></p><p></p></span></div><div><div class="ContentItem-time"><a target="_blank" href="//www.zhihu.com/question/264189719/answer/279814788"><span data-tooltip="发布于 2017-12-20 15:43">发布于 2017-12-20</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 34 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 34</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><div class="Popover ContentItem-action"><button aria-label="更多" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content" type="button" class="Button OptionsButton Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div><div class="ModalWrap"><div><div class=""></div><div class="ModalExp-content"><div class="ModalWrap-content"><div class="ModalWrap-title">继续浏览内容</div><div class="ModalWrap-item"><div class="ModalWrap-itemImg"><img src="https://pic4.zhimg.com/80/v2-88158afcff1e7f4b8b00a1ba81171b61_720w.png" alt=""/></div><div class="ModalWrap-itemContent"><div class="ModalWrap-itemTitle">知乎</div><div class="ModalWrap-itemDesc">发现更大的世界</div></div><div class="ModalWrap-itemBtn">打开</div></div><div class="ModalWrap-item"><div class="ModalWrap-itemImg"><img src="https://pic2.zhimg.com/80/v2-da7d9e4b6a7ddba507299bcf5a4d0600_1440w.png" alt=""/></div><div class="ModalWrap-itemContent"><div class="ModalWrap-itemTitle">Safari</div></div><div class="ModalWrap-itemBtn">继续</div></div></div></div></div></div><div><div><div class=""></div><div class="ModalLoading-content"><svg width="30" height="30" viewBox="0 0 66 66" xmlns="http://www.w3.org/2000/svg" class="CircleLoadingBar" aria-hidden="true"><g><circle class="path" fill="none" stroke-width="6" stroke-linecap="round" cx="33" cy="33" r="30"></circle></g></svg></div></div></div></div></div><div></div></div></div></div></div></div></div></div></div></div></div></main><div data-zop-usertoken="{}"></div></div></div><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","videoHost":"video.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","lens":"https:\u002F\u002Flens.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com","walletpay":"https:\u002F\u002Fwalletpay.zhihu.com","captcha":"https:\u002F\u002Fcaptcha.zhihu.com","vzuu":"https:\u002F\u002Fv.vzuu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"question\u002Fget\u002F":false,"question\u002FgetAnswers\u002F264189719":false}},"club":{"tags":{},"admins":{"data":[]},"members":{"data":[]},"explore":{"candidateSyncClubs":{}},"profile":{},"checkin":{},"comments":{"paging":{},"loading":{},"meta":{},"ids":{}},"postList":{"paging":{},"loading":{},"ids":{}},"recommend":{"data":[]},"silences":{"data":[]},"application":{"profile":null}},"entities":{"users":{"4aa4b84e5469894055272d9917b0ffcb":{"uid":1228851914198470700,"userType":"people","id":"4aa4b84e5469894055272d9917b0ffcb"}},"questions":{"264189719":{"type":"question","id":264189719,"title":"如何理解随机梯度下降（stochastic gradient descent，SGD）？","questionType":"normal","created":1513451182,"updatedTime":1565414282,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F264189719","isMuted":false,"isVisible":true,"isNormal":true,"isEditable":false,"adminClosedComment":false,"hasPublishingDraft":false,"answerCount":16,"visitCount":187436,"commentCount":0,"followerCount":586,"collapsedAnswerCount":0,"excerpt":"它的优缺点在哪？为什么效率比较高？有什么理论支持吗？有实例分析证明收敛性吗？据说在训练 ML、NN 时用的最多，是真的吗？刚接触优化理论，谢谢大家分享！","commentPermission":"all","detail":"\u003Cp\u003E它的优缺点在哪？为什么效率比较高？有什么理论支持吗？有实例分析证明收敛性吗？据说在训练 ML、NN 时用的最多，是真的吗？刚接触优化理论，谢谢大家分享！\u003C\u002Fp\u003E","editableDetail":"\u003Cp\u003E它的优缺点在哪？为什么效率比较高？有什么理论支持吗？有实例分析证明收敛性吗？据说在训练 ML、NN 时用的最多，是真的吗？刚接触优化理论，谢谢大家分享！\u003C\u002Fp\u003E","status":{"isLocked":false,"isClose":false,"isEvaluate":false,"isSuggest":false},"relationship":{"isAuthor":false,"isFollowing":false,"isAnonymous":false,"canLock":false,"canStickAnswers":false,"canCollapseAnswers":false,"voting":0},"topics":[{"id":"19554091","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19554091","name":"数学","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-351d57389cf50b002a20606caac645cf_720w.jpg?source=54b3c3a5","topicType":"NORMAL"},{"id":"19559450","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","name":"机器学习","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-59cdc75217c5bed5676901f99bbf9db4_qhd.jpg","topicType":"NORMAL"},{"id":"19570512","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19570512","name":"优化","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-467ba54bfc1e37dc8ac75bb6ecfca385_720w.jpg?source=54b3c3a5","topicType":"NORMAL"},{"id":"19602355","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19602355","name":"凸优化","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fe82bab09c_l.jpg?source=1940ef5c","topicType":"NORMAL"},{"id":"19607065","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19607065","name":"神经网络","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-f2a3f89b1d244a5315125dd715a14211_720w.jpg?source=54b3c3a5","topicType":"NORMAL"}],"author":{"id":"982fcde0450525c96c18e032c8c5c997","urlToken":"dao-dao-jun","name":"刀刀君","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fe940bec555b98e883a844d7884dc0a2d_l.jpg?source=1940ef5c","avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fe940bec555b98e883a844d7884dc0a2d.jpg?source=1940ef5c","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F982fcde0450525c96c18e032c8c5c997","userType":"people","headline":"学生","badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"gender":1,"isAdvertiser":false,"isPrivacy":false},"canComment":{"status":true,"reason":""},"thumbnailInfo":{"count":0,"type":"thumbnail_info","thumbnails":[]},"reviewInfo":{"type":"","tips":"","editTips":"","isReviewing":false,"editIsReviewing":false},"relatedCards":[],"muteInfo":{"type":""},"showAuthor":false,"isLabeled":false,"isBannered":false,"showEncourageAuthor":false,"voteupCount":18,"canVote":true,"visibleOnlyToAuthor":false}},"answers":{"279814788":{"id":279814788,"type":"answer","answerType":"normal","question":{"type":"question","id":264189719,"title":"如何理解随机梯度下降（stochastic gradient descent，SGD）？","questionType":"normal","created":1513451182,"updatedTime":1565414282,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F264189719","relationship":{}},"author":{"id":"b50035bf95e51ed62d0da7be23644ab9","urlToken":"SofaSofaIO","name":"SofaSofa.io","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-77984d3daa2c48ac9fc3bbfae5ad31d0_l.jpg?source=1940ef5c","avatarUrlTemplate":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-77984d3daa2c48ac9fc3bbfae5ad31d0.jpg?source=1940ef5c","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002Fb50035bf95e51ed62d0da7be23644ab9","userType":"people","headline":"SofaSofa.io做最好的数据科学社区","badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"gender":-1,"isAdvertiser":false,"followerCount":2213,"isFollowed":false,"isPrivacy":false,"vipInfo":{"isVip":false}},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F279814788","isCollapsed":false,"createdTime":1513755786,"updatedTime":1513755786,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":34,"commentCount":0,"isSticky":false,"adminClosedComment":false,"commentPermission":"censor","canComment":{"reason":"你的评论将会由作者筛选后显示","status":true},"reshipmentSettings":"allowed","content":"\u003Cp\u003E谢谢邀请。\u003C\u002Fp\u003E\u003Cp\u003E耐心读完\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fsofasofa.io\u002Ftutorials\u002Fpython_gradient_descent\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E自己动手用python写梯度下降-SofaSofa\u003C\u002Fa\u003E，您就能完全理解了（前面是理论，后面是python实现）。\u003C\u002Fp\u003E\u003Cp\u003E此外，欢迎关注：\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fsofasofa.io\u002Fforum_main_popular.php%3Ff%3D1000005\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E数值计算-热门问题-SofaSofa\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E谢谢阅读！\u003C\u002Fp\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002FSofaSofa.io\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ESofaSofa.io\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E","editableContent":"","excerpt":"谢谢邀请。 耐心读完 自己动手用python写梯度下降-SofaSofa ，您就能完全理解了（前面是理论，后面是python实现）。此外，欢迎关注： 数值计算-热门问题-SofaSofa 谢谢阅读！ SofaSofa.io","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"contentMark":null,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]},"adAnswer":null},"286129370":{"id":286129370,"type":"answer","answerType":"normal","question":{"type":"question","id":264189719,"title":"如何理解随机梯度下降（stochastic gradient descent，SGD）？","questionType":"normal","created":1513451182,"updatedTime":1565414282,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F264189719","relationship":{}},"author":{"id":"7e569555eb46d94c460d4e7e67c10cdb","urlToken":"may-76-28-80","name":"妹妮","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0e0c12438025ab01edbea1b4cf06ed29_l.jpg?source=1940ef5c","avatarUrlTemplate":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0e0c12438025ab01edbea1b4cf06ed29.jpg?source=1940ef5c","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F7e569555eb46d94c460d4e7e67c10cdb","userType":"people","headline":"","badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"gender":0,"isAdvertiser":false,"followerCount":2,"isFollowed":false,"isPrivacy":false,"vipInfo":{"isVip":false}},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F286129370","isCollapsed":false,"createdTime":1514647675,"updatedTime":1514647676,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":2,"commentCount":1,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"allowed","content":"随机梯度下降，每次只选了一个样本做梯度下降，减少计算量吧，但也比较容易遇到局部最优值","editableContent":"","excerpt":"随机梯度下降，每次只选了一个样本做梯度下降，减少计算量吧，但也比较容易遇到局部最优值","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"contentMark":null,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]},"adAnswer":null},"291167114":{"id":291167114,"type":"answer","answerType":"normal","question":{"type":"question","id":264189719,"title":"如何理解随机梯度下降（stochastic gradient descent，SGD）？","questionType":"normal","created":1513451182,"updatedTime":1565414282,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F264189719","relationship":{}},"author":{"id":"fdd159ed687dc032c891326b25c75faf","urlToken":"yzhihao","name":"Evan","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f7135f3f439afc5759dec917272c10d7_l.jpg?source=1940ef5c","avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f7135f3f439afc5759dec917272c10d7.jpg?source=1940ef5c","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002Ffdd159ed687dc032c891326b25c75faf","userType":"people","headline":"欢迎关注我的专栏","badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"gender":1,"isAdvertiser":false,"followerCount":10140,"isFollowed":false,"isPrivacy":false,"vipInfo":{"isVip":false}},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F291167114","isCollapsed":false,"createdTime":1515330886,"updatedTime":1515335413,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":572,"commentCount":0,"isSticky":false,"adminClosedComment":false,"commentPermission":"censor","canComment":{"reason":"你的评论将会由作者筛选后显示","status":true},"reshipmentSettings":"allowed","content":"\u003Cp\u003E因为觉得之前的回答都不是很直接和全面，在下回答一波：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E理解随机梯度下降，首先要知道梯度下降法，故先介绍梯度下降法：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Ch2\u003E梯度下降法\u003C\u002Fh2\u003E\u003Cp\u003E大多数机器学习或者深度学习算法都涉及某种形式的优化。 优化指的是改变 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x+\" alt=\"x \" eeimg=\"1\"\u002F\u003E 以最小化或最大化某个函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"\u002F\u003E 的任务。 我们通常以最小化 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"\u002F\u003E 指代大多数最优化问题。 最大化可经由最小化算法最小化 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=-f%28x%29\" alt=\"-f(x)\" eeimg=\"1\"\u002F\u003E 来实现。\u003C\u002Fp\u003E\u003Cp\u003E我们把要最小化或最大化的函数称为\u003Cb\u003E目标函数或准则\u003C\u002Fb\u003E。 当我们对其进行最小化时，我们也把它称为\u003Cb\u003E代价函数、损失函数或误差函数\u003C\u002Fb\u003E。 \u003C\u002Fp\u003E\u003Cp\u003E下面，我们假设一个损失函数为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29%3D%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%28%7Bh_%5Ctheta%28x%29-y%7D%29%5E2\" alt=\"J(\\theta)=\\frac{1}{2}\\sum_{i=1}^{m}({h_\\theta(x)-y})^2\" eeimg=\"1\"\u002F\u003E ，其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h_%CE%B8%28x%29%3D%CE%B8_0%2B%CE%B8_1x_1%2B%CE%B8_%7B2%7Dx_2....%2B%CE%B8_%7Bn%7Dx_n\" alt=\"h_θ(x)=θ_0+θ_1x_1+θ_{2}x_2....+θ_{n}x_n\" eeimg=\"1\"\u002F\u003E 然后要使得最小化它。\u003C\u002Fp\u003E\u003Cblockquote\u003E注意：这里只是假设，不用知道这个目标函数就是平方损失函数等等，然后肯定有人问既然要最小化它，那求个导数，然后使得导数等于0求出不就好了吗？Emmmm...是的，有这样的解法，可以去了解正规方程组求解。说下这里不讲的原因，主要是那样的方式太难求解，然后在高维的时候，可能不可解，但机器学习或深度学习中，很多都是超高维的，所以也一般不用那种方法。总之，梯度下降是另一种优化的不错方式，比直接求导好很多。\u003C\u002Fblockquote\u003E\u003Cp\u003E\u003Cb\u003E梯度下降：\u003C\u002Fb\u003E我们知道曲面上方向导数的最大值的方向就代表了梯度的方向，因此我们在做梯度下降的时候，应该是沿着梯度的反方向进行权重的更新，可以有效的找到全局的最优解。这个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta_i\" alt=\"\\theta_i\" eeimg=\"1\"\u002F\u003E 的更新过程可以描述为\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-b8845b3f88cb39bc2ecf2eabefa9d7b4_hd.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb\" width=\"480\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b8845b3f88cb39bc2ecf2eabefa9d7b4_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;480&#39; height=&#39;313&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"480\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b8845b3f88cb39bc2ecf2eabefa9d7b4_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-b8845b3f88cb39bc2ecf2eabefa9d7b4_hd.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E[a表示的是步长或者说是学习率（learning rate）]\u003C\u002Fp\u003E\u003Cp\u003E好了，怎么理解？在直观上，我们可以这样理解，看下图，一开始的时候我们随机站在一个点，把他看成一座山，每一步，我们都以下降最多的路线来下山，那么，在这个过程中我们到达山底（最优点）是最快的，而上面的a，它决定了我们“向下山走”时每一步的大小，过小的话收敛太慢，过大的话可能错过最小值，扯到蛋...）。这是一种很自然的算法，每一步总是寻找使J下降最“陡”的方向（就像找最快下山的路一样）。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-77ab0b5fab734c998db19771ed81a0dc_hd.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"641\" data-rawheight=\"312\" class=\"origin_image zh-lightbox-thumb\" width=\"641\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-77ab0b5fab734c998db19771ed81a0dc_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;641&#39; height=&#39;312&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"641\" data-rawheight=\"312\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"641\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-77ab0b5fab734c998db19771ed81a0dc_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-77ab0b5fab734c998db19771ed81a0dc_hd.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E当然了，我们直观上理解了之后，接下来肯定是从数学的角度，我们可以这样想，先想在低维的时候，比如二维，我们要找到最小值，其实可以是这样的方法，具体化到1元函数中时，梯度方向首先是沿着曲线的切线的，然后取切线向上增长的方向为梯度方向，2元或者多元函数中，梯度向量为函数值f对每个变量的导数，该向量的方向就是梯度的方向，当然向量的大小也就是梯度的大小。现在假设我们要求函数的最值，采用梯度下降法，结合如图所示：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-982a9e8f26297deff656990e049383b8_hd.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1022\" data-rawheight=\"774\" class=\"origin_image zh-lightbox-thumb\" width=\"1022\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-982a9e8f26297deff656990e049383b8_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1022&#39; height=&#39;774&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1022\" data-rawheight=\"774\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1022\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-982a9e8f26297deff656990e049383b8_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-982a9e8f26297deff656990e049383b8_hd.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cblockquote\u003E如图所示，我们假设函数是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=y%3Dx%5E2%2B1\" alt=\"y=x^2+1\" eeimg=\"1\"\u002F\u003E ,那么如何使得这个函数达到最小值呢，简单的理解，就是对x求导，得到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=y%E2%80%98%3D%5Cfrac%7B1%7D%7B2%7Dx\" alt=\"y‘=\\frac{1}{2}x\" eeimg=\"1\"\u002F\u003E ，然后用梯度下降的方式，如果初始值是（0的左边）负值,那么这是导数也是负值，用梯度下降的公式，使得x更加的靠近0,如果是正值的时候同理。\u003Cb\u003E注意：这里的梯度也就是一元函数的导数，高维的可以直接类推之\u003C\u002Fb\u003E\u003C\u002Fblockquote\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E然后是优缺点，这里比较对象是批量梯度和mini-batch梯度下降，先看下他们三者：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E批量梯度下降：在每次更新时用所有样本，要留意，在梯度下降中，对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta_i\" alt=\"\\theta_i\" eeimg=\"1\"\u002F\u003E 的更新，所有的样本都有贡献，也就是参与调整 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"\u002F\u003E .其计算得到的是一个标准梯度，\u003Cb\u003E对于最优化问题，凸问题，\u003C\u002Fb\u003E也肯定可以达到一个全局最优。因而理论上来说一次更新的幅度是比较大的。如果样本不多的情况下，当然是这样收敛的速度会更快啦。但是很多时候，样本很多，更新一次要很久，这样的方法就不合适啦。下图是其更新公式\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-5809743fd06c4ff804753d29e4b83935_hd.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"620\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb\" width=\"620\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5809743fd06c4ff804753d29e4b83935_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;620&#39; height=&#39;148&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"620\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"620\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5809743fd06c4ff804753d29e4b83935_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-5809743fd06c4ff804753d29e4b83935_hd.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cul\u003E\u003Cli\u003E随机梯度下降：在每次更新时用1个样本，可以看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整\u003Ci\u003Eθ\u003C\u002Fi\u003E，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，\u003Cb\u003E对于最优化问题，凸问题，\u003C\u002Fb\u003E虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。但是相比于批量梯度，这样的方法更快，更快收敛，虽然不是全局最优，但很多时候是我们可以接受的，所以这个方法用的也比上面的多。下图是其更新公式：\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-b3f14a09ad27df9c66a3af208060f5d7_hd.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"616\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb\" width=\"616\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b3f14a09ad27df9c66a3af208060f5d7_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;616&#39; height=&#39;230&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"616\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"616\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b3f14a09ad27df9c66a3af208060f5d7_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-b3f14a09ad27df9c66a3af208060f5d7_hd.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cul\u003E\u003Cli\u003Emini-batch梯度下降：在每次更新时用b个样本,其实批量的梯度下降就是一种折中的方法，他用了一些小样本来近似全部的，其本质就是我1个指不定不太准，那我用个30个50个样本那比随机的要准不少了吧，而且批量的话还是非常可以反映样本的一个分布情况的。在深度学习中，这种方法用的是最多的，因为这个方法收敛也不会很慢，收敛的局部最优也是更多的可以接受！\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-96181aa7bc39a7e5fb54cfdf2c42a7b9_hd.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"373\" data-rawheight=\"292\" class=\"content_image\" width=\"373\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;373&#39; height=&#39;292&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"373\" data-rawheight=\"292\" class=\"content_image lazy\" width=\"373\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-96181aa7bc39a7e5fb54cfdf2c42a7b9_hd.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E                                }\u003C\u002Fp\u003E\u003Cp\u003E了解之后，总的来说，随机梯度下降一般来说效率高，收敛到的路线曲折，但一般得到的解是我们能够接受的，\u003Cb\u003E在深度学习中，用的比较多的是mini-batch梯度下降。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E最后是收敛性，能收敛吗？收敛到什么地方？\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E对于收敛性的问题，知乎上就有这个问题：\u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fquestion\u002F27012077\" class=\"internal\"\u003E为什么随机梯度下降方法能够收敛？\u003C\u002Fa\u003E，我比较赞赏\u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fquestion\u002F27012077\u002Fanswer\u002F122359602\" class=\"internal\"\u003E李文哲博士的回答\u003C\u002Fa\u003E（推荐一看），总的来说就是从expected loss用特卡洛（monte carlo）来表示计算，那batch GD, mini-batch GD, SGD都可以看成SGD的范畴。因为大家都是在一个真实的分布中得到的样本，对于分布的拟合都是近似的。那这个时候三种方式的梯度下降就都是可以看成用样本来近似分布的过程，都是可以收敛的！\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E对于收敛到什么地方：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E能到的地方：最小值，极小值，鞍点。这些都是能收敛到的地方，也就是梯度为0的点。\u003C\u002Fp\u003E\u003Cp\u003E当然，几乎不存在找到鞍点的可能，除非很碰巧，因为梯度下降是对损失函数每个维度分别求极小值，即分别求 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%CE%B8%29\" alt=\"J(θ)\" eeimg=\"1\"\u002F\u003E 关于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%CE%B8_1%E3%80%81%CE%B8_2...%CE%B8_n\" alt=\"θ_1、θ_2...θ_n\" eeimg=\"1\"\u002F\u003E 极小值。\u003C\u002Fp\u003E\u003Cp\u003E然后是最小值和极小值，如果是\u003Cb\u003E凸函数\u003C\u002Fb\u003E，梯度下降会收敛到最小值，因为只有一个极小值，它就是最小值。\u003C\u002Fp\u003E\u003Cblockquote\u003E至于什么是凸函数，详见我的专栏文章：\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F30486793\" class=\"internal\"\u003E掌握机器学习数学基础之凸优化\u003C\u002Fa\u003E。\u003C\u002Fblockquote\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E对于理论支持：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003EOptimization Methods for Large-Scale Machine Learning：这论文之前的问答也看到了，贴下知友的翻译。\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F28060786\" class=\"internal\"\u003E为什么我们更宠爱“随机”梯度下降？\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003EROBUST STOCHASTIC APPROXIMATION APPROACH TO STOCHASTIC PROGRAMMING\u003C\u002Fp\u003E\u003Cp\u003EAn Introduction to optimization\u003C\u002Fp\u003E\u003Cp\u003E以上三个关于优化的文章，一切问题，自然随之而解。值得一看！\u003C\u002Fp\u003E","editableContent":"","excerpt":"因为觉得之前的回答都不是很直接和全面，在下回答一波： 理解随机梯度下降，首先要知道梯度下降法，故先介绍梯度下降法：梯度下降法大多数机器学习或者深度学习算法都涉及某种形式的优化。 优化指的是改变 [公式] 以最小化或最大化某个函数 [公式] 的任务。 我们通常以最小化 [公式] 指代大多数最优化问题。 最大化可经由最小化算法最小化 [公式] 来实现。我们把要最小化或最大化的函数称为 目标函数或准则。 当我们对其进行最小化时，我…","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"contentMark":null,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]},"adAnswer":null},"649129090":{"id":649129090,"type":"answer","answerType":"normal","question":{"type":"question","id":264189719,"title":"如何理解随机梯度下降（stochastic gradient descent，SGD）？","questionType":"normal","created":1513451182,"updatedTime":1565414282,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F264189719","relationship":{}},"author":{"id":"36f69162230003d316d0b8a6d8da20ba","urlToken":"liang-zi-wei-48","name":"量子位","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ca6e7ffc10a0d10edbae635cee82d007_l.jpg?source=1940ef5c","avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ca6e7ffc10a0d10edbae635cee82d007.jpg?source=1940ef5c","isOrg":true,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F36f69162230003d316d0b8a6d8da20ba","userType":"organization","headline":"有趣的AI&amp;前沿科技→_→ 公众号：QbitAI","badge":[{"type":"identity","description":"已认证的官方帐号","topics":[]},{"type":"best_answerer","description":"优秀答主","topics":[{"id":"19551275","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19551275","name":"人工智能","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-c41d10d22173d515740c43c70f885705_720w.jpg?source=54b3c3a5","topicType":"NORMAL"},{"id":"19556664","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19556664","name":"科技","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002F6d7dc1d82127396331a952e93fda484e_720w.jpg?source=54b3c3a5","topicType":"NORMAL"},{"id":"19556895","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19556895","name":"科研","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-f94b3093434c09b4501b056d142025e0_720w.jpg?source=54b3c3a5","topicType":"NORMAL"},{"id":"19554091","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19554091","name":"数学","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-351d57389cf50b002a20606caac645cf_720w.jpg?source=54b3c3a5","topicType":"NORMAL"},{"id":"19635352","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19635352","name":"自动驾驶","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-a4deab1b73004d7c3fda759c7942c53f_qhd.jpg","topicType":"NORMAL"}]}],"badgeV2":{"title":"2020 新知答主","mergedBadges":[{"type":"best","detailType":"best","title":"新知答主","description":"2020 新知答主","url":"","sources":[],"icon":"","nightIcon":""},{"type":"identity","detailType":"identity","title":"认证","description":"已认证的官方帐号","url":"https:\u002F\u002Fwww.zhihu.com\u002Faccount\u002Fverification\u002Fintro","sources":[],"icon":"","nightIcon":""}],"detailBadges":[{"type":"reward","detailType":"zhihu_yearly_answerer","title":"新知答主","description":"2020 新知答主","url":"","sources":[{"id":"2020","token":"","type":"year","url":"","name":"","avatarPath":"","avatarUrl":"","description":"","priority":0}],"icon":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf0eec3c31c8e866c468f60eb296696c_l.png","nightIcon":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c724649168d8f9d36d7c3d13140a2594_l.png"},{"type":"best","detailType":"best_answerer","title":"优秀答主","description":"数学等 5 个话题下的优秀答主","url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F20054793","sources":[{"id":"19554091","token":"19554091","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F19554091","name":"数学","avatarPath":"v2-351d57389cf50b002a20606caac645cf","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-351d57389cf50b002a20606caac645cf_hd.jpg","description":"","priority":0},{"id":"19556664","token":"19556664","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F19556664","name":"科技","avatarPath":"6d7dc1d82127396331a952e93fda484e","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002F6d7dc1d82127396331a952e93fda484e_hd.jpg","description":"","priority":0},{"id":"19556895","token":"19556895","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F19556895","name":"科研","avatarPath":"v2-f94b3093434c09b4501b056d142025e0","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f94b3093434c09b4501b056d142025e0_hd.jpg","description":"","priority":0},{"id":"19635352","token":"19635352","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F19635352","name":"自动驾驶","avatarPath":"v2-a4deab1b73004d7c3fda759c7942c53f","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-a4deab1b73004d7c3fda759c7942c53f_hd.jpg","description":"","priority":0},{"id":"19551275","token":"19551275","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Ftopic\u002F19551275","name":"人工智能","avatarPath":"v2-c41d10d22173d515740c43c70f885705","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c41d10d22173d515740c43c70f885705_hd.jpg","description":"","priority":0}],"icon":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf0eec3c31c8e866c468f60eb296696c_l.png","nightIcon":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c724649168d8f9d36d7c3d13140a2594_l.png"},{"type":"identity","detailType":"identity_org","title":"已认证的官方帐号","description":"已认证的官方帐号","url":"https:\u002F\u002Fwww.zhihu.com\u002Faccount\u002Fverification\u002Fintro","sources":[],"icon":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-235258cecb8a0f184c4d38483cd6f6b6_l.png","nightIcon":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-45e870b8f0982bcd7537ea4627afbd00_l.png"}],"icon":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-7a1a13d7531f29551f79278e9391b8ee_l.png","nightIcon":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-af71f641951fd5f8b4a7d305288693df_l.png"},"gender":-1,"isAdvertiser":false,"followerCount":378415,"isFollowed":false,"isPrivacy":false,"vipInfo":{"isVip":false}},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F649129090","isCollapsed":false,"createdTime":1555062706,"updatedTime":1555062728,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":251,"commentCount":23,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"allowed","content":"\u003Cp\u003E下面的这张动图演示，乍看就像就像是在复杂地形中作战的沙盘推演，其实揭示的是随机梯度下降（SGD）算法的本质。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-114ae12e4eb14c9c83dddd9e93b31b56_hd.gif?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-thumbnail=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-114ae12e4eb14c9c83dddd9e93b31b56_hd.jpg?source=1940ef5c\" class=\"content_image\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-thumbnail=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-114ae12e4eb14c9c83dddd9e93b31b56_hd.jpg?source=1940ef5c\" class=\"content_image lazy\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-114ae12e4eb14c9c83dddd9e93b31b56_hd.gif?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E让小球滚下山坡，找到它们分别落在哪个山谷里，原来\u003Cb\u003E梯度下降算法\u003C\u002Fb\u003E还能变得像游戏视频一样酷炫。\u003C\u002Fp\u003E\u003Cp\u003E谷歌大脑东京研究员hardmaru转发了视频对应的文章，评价它“像极了即时战略游戏”。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-d558d15ec561096f826882775e7939bc_hd.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d558d15ec561096f826882775e7939bc_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1080&#39; height=&#39;496&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d558d15ec561096f826882775e7939bc_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-d558d15ec561096f826882775e7939bc_hd.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E可别光顾着好玩，视频还得配合文章一起“服用”才有效果。上面的热门视频就是摘自fast.ai成员Javier Ideami写的一篇科普文。\u003C\u002Fp\u003E\u003Cp\u003E如果代码和公式让你感到枯燥，那么不妨从这段酷炫的SGD视频入手，再读一读这篇文章，它会帮你更直观地理解深度学习。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E梯度下降算法的可视化\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E到底什么是梯度？\u003C\u002Fp\u003E\u003Cp\u003E深度学习的架构和最新发展，包括CNN、RNN、造出无数假脸的GAN，都离不开梯度下降算法。\u003C\u002Fp\u003E\u003Cp\u003E梯度可以理解成山坡上某一点上升最快的方向，它的反方向就是下降最快的方向。想要以最快的方式下山，就沿着梯度的反方向走。\u003C\u002Fp\u003E\u003Cp\u003E看起来像沙盘推演的东西，其实是我们撒出的小球，它们会沿着梯度下降的方向滚到谷底。\u003C\u002Fp\u003E\u003Cp\u003E而梯度下降算法的最终目的，是找到整个“地形”中的最低点（全局最小值），也就是海拔最低的山谷。\u003C\u002Fp\u003E\u003Cp\u003E但在这片地形中，山谷可能不止一处（局部最小值），所以我们需要撒很多球，让它们分别落入不同山谷，最后对比高度找到其中的海拔最低点。\u003C\u002Fp\u003E\u003Cp\u003E以上就是随机梯度下降（SGD）算法的基本思想。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E神经网络的输入输出\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E说完梯度下降算法，下面开始介绍神经网络的基本知识。\u003C\u002Fp\u003E\u003Cp\u003E从本质上讲，神经网络是通过一系列“权重” 将输入数据变成我们所需的输出。\u003C\u002Fp\u003E\u003Cp\u003E我们先从最简单的2层神经网络说起，简单介绍一下神经网络的结构。实际上神经网络比这个要复杂得多，通常有几十层甚至上百层。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-eaac0d3b338ffe19479a87593ce3ecca_hd.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"962\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-eaac0d3b338ffe19479a87593ce3ecca_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1080&#39; height=&#39;962&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"962\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-eaac0d3b338ffe19479a87593ce3ecca_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-eaac0d3b338ffe19479a87593ce3ecca_hd.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E输入\u003C\u002Fb\u003E：神经网络的输入是源数据，神经元数量与源数据的特征数匹配。上图以4个输入为例。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E第一层\u003C\u002Fb\u003E：这是个隐藏层，包含有许多隐藏的神经元，它们又连接到周围层中的单元。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E第二层\u003C\u002Fb\u003E：第二层也就是最后一层有1个单元，即网络的输出。\u003C\u002Fp\u003E\u003Cp\u003E输入W和输出Z之间是线性关系：\u003C\u002Fp\u003E\u003Cp\u003E对于第一层网络，Z1=W1X+b1，A1=ReLU（Z1），A1是Z1经过激活函数处理后的结果。\u003C\u002Fp\u003E\u003Cp\u003E对于第二层网络，我们将第一层的输出A1作为第二层的输入，Z2=W2A1+b2，A2=Sigmoid（Z2）。\u003C\u002Fp\u003E\u003Cp\u003EW将表示网络层的\u003Cb\u003E权重\u003C\u002Fb\u003E，它代表着网络不同单元之间连接的强度。b代表\u003Cb\u003E偏置项\u003C\u002Fb\u003E，可以为网络提供更大的灵活性。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-d00e23eec5a0cc526163bc54ef490996_hd.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-d00e23eec5a0cc526163bc54ef490996_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1080&#39; height=&#39;816&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-d00e23eec5a0cc526163bc54ef490996_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-d00e23eec5a0cc526163bc54ef490996_hd.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E最终第二层的输出Yh=A2就是这个神经网络最终的输出。\u003C\u002Fp\u003E\u003Cp\u003EYh = A2 = Sigmoid（W2 ReLU（W1 X + b1）+ b2）\u003C\u002Fp\u003E\u003Cp\u003EYh将代表神经网络的预测结果，就是将X输入给网络后产生的输出。\u003C\u002Fp\u003E\u003Cp\u003E在这个方程中，W1、b1、W2、b2是未知数，需要训练网络找到它们的正确值。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E激活函数的选取\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E你也许会注意到，在上面的神经网络中，每层的输出都是在线性运算的结果后加上一个“过滤”。为什么要这样处理呢？\u003C\u002Fp\u003E\u003Cp\u003E现实中输入和输出之间的关系通常并非线性。如果神经网络的架构仅由线性算法组成，那么它很难计算出非线性行为。所以我们要在每层的结尾加上一个\u003Cb\u003E激活函数\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp\u003E不同的激活函数有不同的特点。选取不当会导致梯度变得非常小，就是通常所说的\u003Cb\u003E梯度消失\u003C\u002Fb\u003E问题。\u003C\u002Fp\u003E\u003Cp\u003E另外还存在一种相反的问题，就是\u003Cb\u003E梯度爆炸\u003C\u002Fb\u003E，当梯度值过大时，网络会变得非常不稳定。\u003C\u002Fp\u003E\u003Cp\u003E常见的4种激活函数有：Sigmoid、tanh，ReLU、leaky ReLU，下面简单讨论一下它们的优缺点。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-8d614109beee6de0e33edb8fea4c2fd3_hd.jpg?source=1940ef5c\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"829\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8d614109beee6de0e33edb8fea4c2fd3_r.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1080&#39; height=&#39;829&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"829\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8d614109beee6de0e33edb8fea4c2fd3_r.jpg?source=1940ef5c\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-8d614109beee6de0e33edb8fea4c2fd3_hd.jpg?source=1940ef5c\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E\u003Cb\u003ESigmoid\u003C\u002Fb\u003E\u003Cbr\u002F\u003E\u003C\u002Fh2\u003E\u003Cp\u003E\u003Cb\u003E1\u002F(1+e-x\u003C\u002Fb\u003E)\u003C\u002Fp\u003E\u003Cp\u003E这个函数非常适合将输入分为两类。它的形状很缓和，因此它的梯度能得到很好的控制。\u003C\u002Fp\u003E\u003Cp\u003E主要的缺点是，在极端情况下，函数输出变得非常平坦。这意味着它存在梯度消失的问题。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003Etanh\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E(2 \u002F (1+e-2x)) - 1\u003C\u002Fp\u003E\u003Cp\u003E它与Sigmoid非常相似。函数曲线更陡峭，因此它的输出也将更强大。缺点与Sigmoid类似。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003EReLU\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003Emax（0，x）\u003C\u002Fp\u003E\u003Cp\u003E如果输入大于0，则输出等于输入。否则，输出为0。\u003C\u002Fp\u003E\u003Cp\u003E它的输出范围从0到无穷大。这意味着它的输出可能会变得非常大，可能存在梯度爆炸问题。它还有个问题是左侧完全平坦，可能导致梯度消失。\u003C\u002Fp\u003E\u003Cp\u003EReLU计算简单，是神经网络内层最常用的激活函数。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003ELeaky ReLU\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E将ReLU函数的前半段用0.01x代替。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003Esoftmax\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003Ee-x \u002F Sum(e-x)\u003C\u002Fp\u003E\u003Cp\u003E输出范围介于0和1之间。\u003C\u002Fp\u003E\u003Cp\u003ESoftmax将输入归一化为概率分布。它将输入压缩为0到1范围，就像Sigmoid。\u003C\u002Fp\u003E\u003Cp\u003E它通常在多分类场景中的输出层，Softmax确保每个类的概率之和为1。\u003C\u002Fp\u003E\u003Cp\u003E实际上，神经网络是一系列函数的组合，有一些是线性的，有一些是非线性的，它们共同组成一个复杂的函数，将输入数据连接到我们需要的输出。\u003C\u002Fp\u003E\u003Cp\u003E文章原作者将这个话题分成3部分讨论，更进一步的内容可以去他文末的链接中找到：\u003Cbr\u002F\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Ftowardsdatascience.com\u002Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Etowardsdatascience.com\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Ethe-keys-of-deep-learning-in-100-lines-of-code-907398c76504\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003E— \u003Cb\u003E完\u003C\u002Fb\u003E —\u003Cbr\u002F\u003E量子位 · QbitAI\u003Cbr\u002F\u003Eվ&#39;ᴗ&#39; ի 追踪AI技术和产品新动态\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Forg\u002Fliang-zi-wei-48\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ca6e7ffc10a0d10edbae635cee82d007_ipico.jpg\" data-image-width=\"250\" data-image-height=\"250\" class=\"internal\"\u003E量子位\u003C\u002Fa\u003E\u003Cp\u003E欢迎大家关注我们，以及订阅\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fqbitai\" class=\"internal\"\u003E我们的知乎专栏\u003C\u002Fa\u003E\u003C\u002Fp\u003E","editableContent":"","excerpt":"下面的这张动图演示，乍看就像就像是在复杂地形中作战的沙盘推演，其实揭示的是随机梯度下降（SGD）算法的本质。 [图片] 让小球滚下山坡，找到它们分别落在哪个山谷里，原来 梯度下降算法还能变得像游戏视频一样酷炫。谷歌大脑东京研究员hardmaru转发了视频对应的文章，评价它“像极了即时战略游戏”。 [图片] 可别光顾着好玩，视频还得配合文章一起“服用”才有效果。上面的热门视频就是摘自fast.ai成员Javier Ideami写的一篇科普文。 如果代…","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"contentMark":null,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]},"adAnswer":null},"651492369":{"id":651492369,"type":"answer","answerType":"normal","question":{"type":"question","id":264189719,"title":"如何理解随机梯度下降（stochastic gradient descent，SGD）？","questionType":"normal","created":1513451182,"updatedTime":1565414282,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F264189719","relationship":{}},"author":{"id":"60180fde0280d3255cfe3798fee38802","urlToken":"li-ao-shi","name":"肌肉铠甲","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-8d6d5bbd7546028199492bc17723be46_l.jpg?source=1940ef5c","avatarUrlTemplate":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-8d6d5bbd7546028199492bc17723be46.jpg?source=1940ef5c","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F60180fde0280d3255cfe3798fee38802","userType":"people","headline":"天地不仁，君子以自强不息","badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"gender":1,"isAdvertiser":false,"followerCount":206,"isFollowed":false,"isPrivacy":false,"vipInfo":{"isVip":false}},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F651492369","isCollapsed":false,"createdTime":1555264407,"updatedTime":1555412303,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":18,"commentCount":5,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"allowed","content":"\u003Cp\u003E我之前不理解随机梯度下降的主要原因是，假设一个mini batch两个样本：(Xi, yi), (Xj, yj)\u003C\u002Fp\u003E\u003Cp\u003E那我算出这两个样本的导数之后取平均对么？直观的想法比如对于一个函数y = f(X)，如果我任取两个X，那么这两点在f(X)上的斜率肯定是不一样的，为什么可以取平均呢？不应该是计算一个点的斜率，然后再从该点梯度下降吗？\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E这里是因为我搞混淆了一个重要的概念，神经网络是关于参数W的函数，而非关于训练数据的函数。把损失函数写一下就明白了，我们的目标是找到一个W，使得L(W)尽可能小，也就是最好W的值在f(W)的斜率为0（局部最优，最优，还有鞍点。多谢\u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002Fdu-chen-zhuang-66\" class=\"internal\"\u003E杜晨壮\u003C\u002Fa\u003E指正！）的点上。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E而L(W) 以回归问题为例，是(y预测 - y实际)关于每一个样本的均方。所以，取两个样本的时候，只要控制当前的W不变，对应的就是L(W)图形曲线上的一个点，而非两个点。对于这个点求梯度是有意义的。相当于就是想象训练样本数变少了而已，多训练几轮嘛。\u003C\u002Fp\u003E","editableContent":"","excerpt":"我之前不理解随机梯度下降的主要原因是，假设一个mini batch两个样本：(Xi, yi), (Xj, yj) 那我算出这两个样本的导数之后取平均对么？直观的想法比如对于一个函数y = f(X)，如果我任取两个X，那么这两点在f(X)上的斜率肯定是不一样的，为什么可以取平均呢？不应该是计算一个点的斜率，然后再从该点梯度下降吗？ 这里是因为我搞混淆了一个重要的概念，神经网络是关于参数W的函数，而非关于训练数据的函数。把损失函数写一下就明白…","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"contentMark":null,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]},"adAnswer":null}},"articles":{},"columns":{},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{},"zvideos":{},"zvideoContributions":{}},"currentUser":"4aa4b84e5469894055272d9917b0ffcb","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false},"cardUserInfo":{"vipInfo":{}},"handleWidget":{},"widgetList":[],"userWidgetId":""},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{},"infinity":{},"batchUsers":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-li_video-3","expPrefix":"li_video","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_art2qa_new-3","expPrefix":"qa_art2qa_new","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_cl_guest-2","expPrefix":"qa_cl_guest","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_item-3","expPrefix":"se_item","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-tp_zrec-8","expPrefix":"tp_zrec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_adjust_new-11","expPrefix":"us_adjust_new","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_noti_count-8","expPrefix":"us_noti_count","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_andplay_d-2","expPrefix":"vd_andplay_d","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_hookupplay_an-2","expPrefix":"vd_hookupplay_an","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_ios_play-10","expPrefix":"vd_ios_play","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_timeguide-2","expPrefix":"vd_timeguide","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_video_replay-3","expPrefix":"vd_video_replay","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-zanswer-2","expPrefix":"zanswer","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-edu_cd1-3","expPrefix":"edu_cd1","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vdox_cmt-2","expPrefix":"vdox_cmt","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-hwtj_bottom_btn-4","expPrefix":"hwtj_bottom_btn","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-km_detail_knn-2","expPrefix":"km_detail_knn","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-recnew_2th-3","expPrefix":"recnew_2th","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_meta_ss-3","expPrefix":"se_meta_ss","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"qa_cdzixun-1","expPrefix":"qa_cdzixun","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"qa_np-1","expPrefix":"qa_np","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"qa_fo_recom_h-3","expPrefix":"qa_fo_recom_h","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"qa_recmessa-1","expPrefix":"qa_recmessa","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"vd_video_tab-9","expPrefix":"vd_video_tab","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"tp_content-5","expPrefix":"tp_content","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"office_xp-3_v2","expPrefix":"office_xp","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"xuanpinab-2_v4","expPrefix":"xuanpinab","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"editor-2_v5","expPrefix":"editor","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"pc_editorplugin-2_v10","expPrefix":"pc_editorplugin","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"use_bff_profit-1_v7","expPrefix":"use_bff_profit","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"creator_hot_rec-2_v2","expPrefix":"creator_hot_rec","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"userlinkv2-2_v9","expPrefix":"userlinkv2","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"h5_signin-2_v3","expPrefix":"h5_signin","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"new_ques_rec-4_v1","expPrefix":"new_ques_rec","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"meta_ebook-2_v2","expPrefix":"meta_ebook","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"webpImg-2_v3","expPrefix":"webpImg","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"ffzix_xgdzwz-1_v1","expPrefix":"ffzix_xgdzwz","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"rec_new2th-2_v4","expPrefix":"rec_new2th","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"hw_aa_30-1_v2","expPrefix":"hw_aa_30","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"hw_aa_50-1_v1","expPrefix":"hw_aa_50","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"ad_com_zhi-2_v3","expPrefix":"ad_com_zhi","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"ge_v_rank_v3-3_v6","expPrefix":"ge_v_rank_v3","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_dwd_all-12_v3","expPrefix":"se_dwd_all","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"test_3-2_v2","expPrefix":"test_3","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_mixer_km-7_v18","expPrefix":"se_mixer_km","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"book_kpwz-3_v2","expPrefix":"book_kpwz","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"rec_3th-3_v3","expPrefix":"rec_3th","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_cp_question-4_v6","expPrefix":"se_cp_question","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_cp_answer-4_v5","expPrefix":"se_cp_answer","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_cp_post3-2_v3","expPrefix":"se_cp_post3","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"account_sdk-1_v9","expPrefix":"account_sdk","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_feat-4_v6","expPrefix":"se_feat","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_zhiplus_ecpm-1_v14","expPrefix":"se_zhiplus_ecpm","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_guess_hot-3_v2","expPrefix":"se_guess_hot","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_tabcombine-3_v2","expPrefix":"se_tabcombine","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"recommend_list-3_v1","expPrefix":"recommend_list","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_train_data-1_v8","expPrefix":"se_train_data","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"explore_newbee-2_v2","expPrefix":"explore_newbee","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"hottiming_pc-2_v4","expPrefix":"hottiming_pc","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_min_recall-3_v1","expPrefix":"se_min_recall","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"gue_q_share","type":"String","value":"0","layerId":"gueqa_layer_647"},{"id":"gue_fo_recom","type":"String","value":"1","layerId":"gueqa_layer_780"},{"id":"web_column_auto_invite","type":"String","value":"0","layerId":"webqa_layer_1"},{"id":"gue_sharp","type":"String","value":"1","layerId":"guevd_layer_686"},{"id":"ad_com_zhi","type":"Int","value":"0","chainId":"_gene_","layerId":"ad_com_zhi","key":571},{"id":"test_3","type":"Int","value":"1","chainId":"_gene_","layerId":"dABe","key":637},{"id":"web_login","type":"String","value":"0","layerId":"webgw_layer_759"},{"id":"club_fn","type":"Int","value":"1","layerId":"club_fn"},{"id":"ge_guess","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_938","key":2912},{"id":"correct_cnn","type":"Int","value":"0","chainId":"_gene_","layerId":"correct_cnn","key":397},{"id":"ge_v_rank_v3","type":"Int","value":"2","chainId":"_gene_","layerId":"FPcB","key":581},{"id":"ge_relation2","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_815","key":2796},{"id":"editor","type":"Int","value":"1","layerId":"editor"},{"id":"creator_hot_rec","type":"Int","value":"2","layerId":"ju92"},{"id":"test_rec","type":"Int","value":"10","layerId":"test_rec"},{"id":"explore_newbee","type":"Int","value":"0","chainId":"_gene_","layerId":"explore_newbee","key":930},{"id":"se_proxi","type":"Int","value":"0","chainId":"_gene_","layerId":"Kv01","key":939},{"id":"web_heifetz_grow_ad","type":"String","value":"1","layerId":"webgw_layer_3"},{"id":"ge_hard_s_ma","type":"String","value":"0","chainId":"_gene_","layerId":"geli_layer_856","key":3031},{"id":"ge_yuzhi_v1","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_1029","key":3127},{"id":"recnew_2th","type":"Int","value":"21","chainId":"_gene_","layerId":"l-recnew_2th","key":67},{"id":"se_cp_question","type":"Int","value":"1","chainId":"_gene_","layerId":"6w98","key":704},{"id":"se_cp_answer","type":"Int","value":"1","chainId":"_gene_","layerId":"WpRW","key":705},{"id":"ge_search_ui","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_838","key":2898},{"id":"pf_noti_entry_num","type":"String","value":"2","chainId":"_all_","layerId":"pfus_layer_718"},{"id":"web_collection_guest","type":"String","value":"1","layerId":"webqa_layer_4"},{"id":"ge_sxzx","type":"String","value":"0","chainId":"_gene_","layerId":"gere_layer_990","key":3060},{"id":"gue_playh_an","type":"String","value":"1","layerId":"guevd_layer_622"},{"id":"plus_panel","type":"Int","value":"0","chainId":"_gene_","layerId":"2CDt","key":818},{"id":"Re_Collection","type":"Int","value":"0","chainId":"_gene_","layerId":"9olk","key":847},{"id":"iserial","type":"Int","value":"0","chainId":"_gene_","layerId":"iserial","key":928},{"id":"ge_video","type":"String","value":"1","chainId":"_gene_","layerId":"geli_layer_856","key":2831},{"id":"gue_profile_video","type":"String","value":"1","layerId":"guevd_layer_5"},{"id":"webpImg","type":"Int","value":"2","layerId":"JnVt"},{"id":"hwtj_bottom_btn","type":"Int","value":"4","layerId":"l-jrhg"},{"id":"rec_new2th","type":"Int","value":"1","chainId":"_gene_","layerId":"Hump","key":320},{"id":"book_kpwz","type":"Int","value":"2","chainId":"_gene_","layerId":"Zyox","key":671},{"id":"se_feat","type":"Int","value":"1","chainId":"_gene_","layerId":"se_feat","key":837},{"id":"xuanpinab","type":"Int","value":"0","layerId":"xuanpinab"},{"id":"use_bff_profit","type":"Int","value":"0","layerId":"use_bff_profit"},{"id":"meta_ebook","type":"Int","value":"1","layerId":"meta_ebook"},{"id":"se_guess_hot","type":"Int","value":"1","chainId":"_gene_","layerId":"1nGW","key":882},{"id":"ge_rec_2th","type":"String","value":"11","chainId":"_gene_","layerId":"geli_layer_965","key":3023},{"id":"recommend_list","type":"Int","value":"1","chainId":"_gene_","layerId":"ejiC","key":921},{"id":"pf_adjust","type":"String","value":"1","chainId":"_all_","layerId":"pfus_layer_9"},{"id":"gue_iosplay","type":"String","value":"1","layerId":"guevd_layer_810"},{"id":"gue_bulletmb","type":"String","value":"0","layerId":"guevd_layer_812"},{"id":"se_images","type":"Int","value":"0","chainId":"_gene_","layerId":"ulTc","key":652},{"id":"se_zhiplus_cpc","type":"Int","value":"0","chainId":"_gene_","layerId":"se_zhiplus_cpc","key":716},{"id":"io_video_tab","type":"Int","value":"0","chainId":"_gene_","layerId":"io_video_tab","key":728},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_","layerId":"zrrec_layer_5"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_","layerId":"tptp_layer_4"},{"id":"hw_aa_50","type":"Int","value":"0","chainId":"_gene_","layerId":"hw_aa_50","key":362},{"id":"Full_ans_fav","type":"Int","value":"0","chainId":"_gene_","layerId":"Pnd6","key":372},{"id":"web_answerlist_ad","type":"String","value":"0","layerId":"webqa_layer_1"},{"id":"show_ad","type":"Int","value":"0","chainId":"_gene_","layerId":"show_ad","key":27},{"id":"hw_aa_30","type":"Int","value":"0","chainId":"_gene_","layerId":"hw_aa_30","key":361},{"id":"ge_usercard1","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_742","key":2740},{"id":"ge_meta_ss","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_834","key":3079},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_","layerId":"sese_layer_4"},{"id":"ge_emoji","type":"String","value":"0","chainId":"_gene_","layerId":"getp_layer_827","key":3209},{"id":"Test_Punk","type":"Int","value":"0","layerId":"Test_Punk"},{"id":"se_train_data","type":"Int","value":"0","chainId":"_gene_","layerId":"se_train_data","key":922},{"id":"se_rel_knn_gen","type":"Int","value":"0","chainId":"_gene_","layerId":"Sih1","key":931},{"id":"h5_signin","type":"Int","value":"1","layerId":"h5_signin"},{"id":"se_4a","type":"Int","value":"0","chainId":"_gene_","layerId":"rtiq","key":335},{"id":"ge_newbie3","type":"Int","value":"0","chainId":"_gene_","layerId":"ge_newbie3","key":180},{"id":"hottiming_pc","type":"Int","value":"0","chainId":"_gene_","layerId":"hottiming_pc","key":961},{"id":"gue_card_test","type":"String","value":"1","layerId":"gueqa_layer_2"},{"id":"gue_vid_tab","type":"String","value":"7","layerId":"guevd_layer_900"},{"id":"userlinkv2","type":"Int","value":"1","layerId":"userlinkv2"},{"id":"pfd_newbie","type":"Int","value":"0","chainId":"_gene_","layerId":"pfd_newbie","key":63},{"id":"pfd_newbie2","type":"Int","value":"0","chainId":"_gene_","layerId":"pfd_newbie2","key":71},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_","layerId":"qapqa_layer_2"},{"id":"an_video_tab","type":"Int","value":"0","chainId":"_gene_","layerId":"an_video_tab","key":727},{"id":"se_cp_post3","type":"Int","value":"0","chainId":"_gene_","layerId":"VZLx","key":706},{"id":"web_audit_01","type":"String","value":"case1","layerId":"webre_layer_1"},{"id":"ge_dipin_pre","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1000","key":3124},{"id":"pc_editorplugin","type":"Int","value":"1","layerId":"pc_editorplugin"},{"id":"edu_cd1","type":"Int","value":"1","layerId":"l-l18Y"},{"id":"km_detail_knn","type":"Int","value":"1","layerId":"l-TY59"},{"id":"gue_video_replay","type":"String","value":"2","layerId":"guevd_layer_3"},{"id":"se_must","type":"Int","value":"0","chainId":"_gene_","layerId":"HJvb","key":876},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_","layerId":"zrrec_layer_11"},{"id":"tp_dingyue_video","type":"String","value":"0","chainId":"_all_","layerId":"tptp_layer_4"},{"id":"se_cvr_boost","type":"Int","value":"0","chainId":"_gene_","layerId":"se_cvr_boost","key":183},{"id":"se_tabcombine","type":"Int","value":"3","chainId":"_gene_","layerId":"se_tabcombine","key":907},{"id":"ge_sug_rep","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_1034","key":3158},{"id":"ge_newcard","type":"String","value":"3","chainId":"_gene_","layerId":"geus_layer_839","key":2997},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…","layerId":"guevd_layer_0"},{"id":"zanswer","type":"Int","value":"1","layerId":"l-zanswer"},{"id":"gue_cdzixun","type":"String","value":"0","layerId":"gueqa_layer_3"},{"id":"ge_prf_rec","type":"String","value":"0","chainId":"_gene_","layerId":"getop_layer_991","key":3040},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_","layerId":"qapqa_layer_2"},{"id":"iosserial","type":"Int","value":"0","chainId":"_gene_","layerId":"m7Id","key":714},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_","layerId":"iosus_layer_1"},{"id":"gue_video_guide","type":"String","value":"1","layerId":"guevd_layer_625"},{"id":"new_ques_rec","type":"Int","value":"4","layerId":"i1NO"},{"id":"web_scl_rec","type":"String","value":"0","layerId":"webgw_layer_759"},{"id":"gue_messrec","type":"String","value":"0","layerId":"gueqa_layer_769"},{"id":"pc_content","type":"Int","value":"0","chainId":"_gene_","layerId":"pc_content","key":910},{"id":"ge_item","type":"String","value":"2","chainId":"_gene_","layerId":"gese_layer_945","key":2971},{"id":"gue_zvideo_link","type":"String","value":"1","layerId":"guevd_layer_2"},{"id":"ge_entity","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_946","key":3036},{"id":"ge_newyanzhi","type":"String","value":"0","chainId":"_gene_","layerId":"geus_layer_1019","key":2788},{"id":"ts_cardtitle","type":"Int","value":"0","layerId":"l-ZED7"},{"id":"se_fix_ebook","type":"Int","value":"0","chainId":"_gene_","layerId":"se_fix_ebook","key":103},{"id":"gue_art2qa","type":"String","value":"0","layerId":"gueqa_layer_579"},{"id":"comment","type":"Int","value":"0","chainId":"_gene_","layerId":"comment","key":644},{"id":"book_ebook","type":"Int","value":"0","chainId":"_gene_","layerId":"book_ebook","key":758},{"id":"gue_v_serial","type":"String","value":"1","layerId":"guevd_layer_695"},{"id":"ffzix_xgdzwz","type":"Int","value":"0","layerId":"ffzix_xgdzwz"},{"id":"click_new_tb","type":"Int","value":"0","chainId":"_gene_","layerId":"1vYQ","key":422},{"id":"se_personal_ab","type":"Int","value":"0","chainId":"_gene_","layerId":"ydUx","key":554},{"id":"And_Collection","type":"Int","value":"0","chainId":"_gene_","layerId":"VbMB","key":848},{"id":"se_zhiplus_ecpm","type":"Int","value":"0","chainId":"_gene_","layerId":"mr6i","key":855},{"id":"gue_goods_card","type":"String","value":"0","layerId":"gueqa_layer_1"},{"id":"tp_contents","type":"String","value":"1","chainId":"_all_","layerId":"tptp_layer_627"},{"id":"ts_refresh","type":"Int","value":"1","layerId":"EBn6"},{"id":"se_mixer_km","type":"Int","value":"4","chainId":"_gene_","layerId":"se_mixer_km","key":649},{"id":"rec_3th","type":"Int","value":"21","chainId":"_gene_","layerId":"pRqt","key":697},{"id":"andserial","type":"Int","value":"0","chainId":"_gene_","layerId":"CAxI","key":711},{"id":"gue_recmess","type":"String","value":"0","layerId":"gueqa_layer_795"},{"id":"web_answer_list_ad","type":"String","value":"1","layerId":"webqa_layer_4"},{"id":"gue_bullet_second","type":"String","value":"1","layerId":"guevd_layer_1"},{"id":"office_xp","type":"Int","value":"7777","layerId":"office_xp"},{"id":"se_dbert","type":"Int","value":"0","chainId":"_gene_","layerId":"se_dbert","key":943},{"id":"gue_art_ui","type":"String","value":"2","layerId":"gueqa_layer_647"},{"id":"ge_infinity6","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_815","key":2817},{"id":"gue_repost","type":"String","value":"1","layerId":"gueqa_layer_671"},{"id":"vdox_cmt","type":"Int","value":"1","layerId":"l-9YBb"},{"id":"account_sdk","type":"Int","value":"1","chainId":"_gene_","layerId":"account_sdk","key":810},{"id":"gue_visit_n_artcard","type":"String","value":"1","layerId":"gueqa_layer_579"},{"id":"android_serial","type":"Int","value":"0","chainId":"_gene_","layerId":"android_serial","key":929},{"id":"tp_zrec","type":"String","value":"1","chainId":"_all_","layerId":"tptp_layer_619"},{"id":"gue_andplayd","type":"String","value":"1","layerId":"guevd_layer_686"},{"id":"se_dwd_all","type":"Int","value":"9","chainId":"_gene_","layerId":"se_dwd_all","key":621},{"id":"se_min_recall","type":"Int","value":"1","chainId":"_gene_","layerId":"JhGr","key":971},{"id":"web_sem_ab","type":"String","value":"1","layerId":"webgw_layer_3"}],"chains":[{"chainId":"_all_"}],"encodedParams":"CpABOwJ9AmALjQFFAuwKogOrA9cLNwxDAMACwQJSC\u002FQLMgNPA6ADDwtAAZ8CRQNyA88LmQOMAswC2AJqAXQBGwBpAbQKBwyJDJoDowNPAbQAwQM\u002FAEcA1wLCAjQMbAO3AIsDVgy1C+ALygKOA5sL3AvkCmcAhAL2AqYBKgJQA1cDiQK5AscCrwMBCyoDoQNtAssDEkgAAQAAAgEAAAABFQEBAQAAAAABAQIBAQsBAAAAAAAAAAABAAAAAAAAAAAAAAAAAAMBAwAAAAIAAAAAAAAAAAAEFQAAAAEACQE="},"triggers":{}},"userAgent":{"Edge":false,"IE":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":true,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":true,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"WxMiniProgram":false,"BaiduMiniProgram":false,"QQMiniProgram":false,"JDMiniProgram":false,"isWebView":false,"isMiniProgram":false,"origin":"Mozilla\u002F5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit\u002F605.1.15 (KHTML, like Gecko) Version\u002F14.0.3 Safari\u002F605.1.15"},"appViewConfig":{},"ctx":{"path":"\u002Fquestion\u002F264189719","query":{},"href":"http:\u002F\u002Fwww.zhihu.com\u002Fquestion\u002F264189719","host":"www.zhihu.com"},"trafficSource":"production","edition":{"beijing":false,"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false,"baiduSearch":false,"googleSearch":false,"miniProgram":false,"xiaomi":false},"theme":"light","enableShortcut":true,"referer":"","xUDId":"APAeLA72HhOPTrRlgzi4R83BfKjwhBDSH1E=","mode":"ssr","conf":{},"xTrafficFreeOrigin":"","ipInfo":{},"logged":true,"vars":{"passThroughHeaders":{}}},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{},"videoSupport":{},"mcnManage":{},"tasks":{},"recentlyCreated":[],"analysis":{"all":{},"answer":{},"zvideo":{},"article":{},"pin":{},"singleContent":{}},"announcement":{},"school":{"tabs":[],"contents":[],"banner":null,"entities":{}}},"question":{"followers":{},"concernedFollowers":{},"answers":{"264189719":{"isFetching":false,"isDrained":false,"ids":[291167114,279814788,649129090,286129370,651492369],"newIds":[291167114,279814788,649129090,286129370,651492369],"totals":16,"antiSpider":false,"error":false,"isPrevDrained":true,"previous":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F264189719\u002Fanswers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cattachment%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Cis_labeled%2Cpaid_info%2Cpaid_info_content%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_recognized%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cvip_info%2Cbadge%5B%2A%5D.topics%3Bdata%5B%2A%5D.settings.table_of_content.enabled&limit=5&offset=0&platform=desktop&sort_by=default","next":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F264189719\u002Fanswers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cattachment%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Cis_labeled%2Cpaid_info%2Cpaid_info_content%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_recognized%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cvip_info%2Cbadge%5B%2A%5D.topics%3Bdata%5B%2A%5D.settings.table_of_content.enabled&limit=5&offset=5&platform=desktop&sort_by=default"}},"hiddenAnswers":{},"updatedAnswers":{},"collapsedAnswers":{},"notificationAnswers":{},"invitedQuestions":{"total":{"count":null,"isEnd":false,"isLoading":false,"questions":[]},"followees":{"count":null,"isEnd":false,"isLoading":false,"questions":[]}},"laterQuestions":{"count":null,"isEnd":false,"isLoading":false,"questions":[]},"waitingQuestions":{"recommend":{"isEnd":false,"isLoading":false,"questions":[]},"hot":{"isEnd":false,"isLoading":false,"questions":[]},"newest":{"isEnd":false,"isLoading":false,"questions":[]},"invite":{"isEnd":false,"isLoading":false,"questions":[]}},"invitationCandidates":{},"inviters":{},"invitees":{},"similarQuestions":{},"questionBanners":{},"relatedCommodities":{},"bio":{},"brand":{},"permission":{},"adverts":{},"advancedStyle":{},"commonAnswerCount":0,"hiddenAnswerCount":0,"meta":{},"bluestarRanklist":{},"relatedSearch":{},"autoInvitation":{},"simpleConcernedFollowers":{},"draftStatus":{},"disclaimers":{},"isShowMobileSignInModal":false},"shareTexts":{},"answers":{"voters":{},"copyrightApplicants":{},"favlists":{},"newAnswer":{},"concernedUpvoters":{},"simpleConcernedUpvoters":{},"paidContent":{},"settings":{}},"banner":{},"topic":{"bios":{},"hot":{},"newest":{},"top":{},"unanswered":{},"questions":{},"followers":{},"contributors":{},"parent":{},"children":{},"bestAnswerers":{},"wikiMeta":{},"index":{},"intro":{},"meta":{},"schema":{},"creatorWall":{},"wikiEditInfo":{},"committedWiki":{},"landingBasicData":{},"landingExcellentItems":[],"landingExcellentEditors":[],"landingCatalog":[],"landingEntries":{}},"explore":{"recommendations":{},"specials":{"entities":{},"order":[]},"roundtables":{"entities":{},"order":[]},"collections":{},"columns":{}},"articles":{"voters":{},"concernedUpvoters":{}},"favlists":{"relations":{}},"pins":{"reviewing":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"room":{"meta":{},"isFetching":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]},"hotDaily":{"data":[],"paging":{}},"hotHighlight":{"isFetching":false,"isDrained":false,"data":[],"paging":{}},"banner":{},"video":{"items":[],"next":null,"isLoading":false,"isDrained":false}},"upload":{},"video":{"data":{},"shareVideoDetail":{},"last":{}},"zvideos":{"campaigns":{},"tagoreCategory":[],"recommendations":{},"insertable":{},"recruit":{"form":{"platform":"","nickname":"","followerCount":"","domain":"","contact":""},"submited":false,"ranking":[]},"club":{},"qyActivityData":{},"talkActivityData":{},"batchVideos":{},"contribution":{"selectedContribution":null,"configs":{},"contributionLists":{},"recommendQuestions":{"isLoading":true,"paging":{"isEnd":false,"isStart":true,"totals":0},"data":[]},"questionSearchResults":{"isLoading":true,"paging":{"isEnd":false,"isStart":true,"totals":0},"data":[]}},"creationReferences":{}},"guide":{"guide":{"isFetching":false,"isShowGuide":false}},"reward":{"answer":{},"article":{},"question":{}},"search":{"recommendSearch":[],"topSearch":{},"searchValue":{},"suggestSearch":{},"attachedInfo":{},"nextOffset":{},"topicReview":{},"calendar":{},"scores":null,"majors":{},"university":{},"generalByQuery":{},"generalByQueryInADay":{},"generalByQueryInAWeek":{},"generalByQueryInThreeMonths":{},"peopleByQuery":{},"clubentityByQuery":{},"clubPostByQuery":{},"topicByQuery":{},"zvideoByQuery":{},"scholarByQuery":{},"columnByQuery":{},"liveByQuery":{},"albumByQuery":{},"eBookByQuery":{},"kmGeneralByQuery":{}},"creatorSalt":{"recommendQuestionList":[],"bannerList":[],"sites":[],"domains":{},"hasRecored":false,"hasClaim":false,"hasContributedList":[],"notContributedList":[],"contributesTotal":null,"isShowClaimModal":false,"previewPageTitle":"","previewPageContent":""},"publicEditPermission":{},"vessay":{"common":{"draftId":null,"source":{"type":null,"id":null},"autoSave":true,"blockUnload":true,"isCalibratingEditing":false,"editingTrackData":{"editingOutlines":[],"editingVideos":[]},"newAddMaterialData":{},"audioCache":{},"showTimbreRecordPanel":false},"loading":{"isLoading":false,"text":"正在加载中"},"player":{"currentTime":0,"totalTime":0,"playing":false},"library":{"material":{},"materialSearchResult":{},"music":{},"musicLibraryCategories":[],"musicUrls":{}},"track":{"trackData":{"dataVersion":1,"videoTrack":[],"audioTrack":[],"musicTrack":[],"voiceOverData":{}},"previewEditingTrackData":{"editingSubtitleItem":{},"editingVideoItem":{}},"selectedTrackItems":[],"outlineRemoveMaterial":{},"outlineAppliedStyle":{},"timbres":[],"timbreId":"","timbreTests":[],"newRecordBlob":{},"exportErrorUrls":[]}},"readStatus":{},"draftHistory":{"history":{},"drafts":{}},"notifications":{"recent":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"history":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"notificationActors":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"recentNotificationEntry":"all"},"specials":{"entities":{},"all":{"data":[],"paging":{},"isLoading":false}},"collections":{"hot":{"data":[],"paging":{},"isLoading":false},"collectionFeeds":{}},"userProfit":{"permission":{"permissionStatus":{"zhiZixuan":0,"recommend":-1,"task":0,"plugin":0,"infinity":0},"visible":false}},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[],"lists":{},"banners":{},"protocolStatus":{"isAgreedNew":true,"isAgreedOld":true},"probationCountdownDays":0},"mcnActivity":{"household":{"products":{},"rankList":{"total":{},"yesterday":{}}}},"brand":{"contentPlugin":{}},"host":{"roundtable":{"subjects":{},"applications":{"total":0},"online":{"total":0},"applies":{},"details":{},"includedResource":{},"hotQuestions":{},"warmupContents":{},"batchInclude":{}},"special":{"applications":{"total":0,"pages":{},"entities":{}},"censorHistory":{},"drafts":{}}},"campaign":{"single":{},"list":{},"videoMakerAcq":{},"vote":{},"cardCollecting":{"message":null,"profile":{"balance":"0","chance":0,"coinNum":0,"gatherClose":false,"isGotMagicCard":false,"isPay":false,"partitionStart":false,"totalDone":0,"withdrawStart":false},"sharePoster":{"share":"","sendCard":"","invite":""},"shareLink":null,"shareIntention":"share","shareKey":null,"shareCardId":null,"inviterInfo":null,"giverInfo":null,"prize":null,"receivedCard":null,"newCoinCount":null,"newCardList":[],"newUserCardCount":1,"taskList":[],"prizeList":null,"cardList":null,"panel":{"showTaskPanel":false,"showRewardPanel":false},"modal":{"showWelcomeModal":false,"showFusionModal":false,"showFusionPromptModal":false,"showShareModal":false,"showBackModal":false}},"zhiboPandian2020":null,"boarding":{},"searchGaokaoSubPage":{},"searchHealth":{},"superBrain":{"info":{"national_rank_list":[],"child_rank_list":[],"user_info":{"wisdomScore":0}},"rank":{"items":[],"isLoading":false,"isDrained":false},"search":[]}},"knowledgePlan":{"lists":{},"allCreationRankList":{},"featuredQuestions":{}},"wallE":{"protectHistory":{"total":0,"pages":{},"entities":{}}},"roundtables":{"hotQuestions":{},"warmupContents":{},"hotDiscussions":{},"selectedContents":{},"roundtables":{}},"helpCenter":{"entities":{"question":{},"category":{}},"categories":[],"commonQuestions":[],"relatedQuestions":{}},"republish":{},"commercialReport":{"commercialTypes":[]},"creatorMCN":{"mcn":{},"mcnStatistics":{},"isNoAuth":false,"creatorManageData":[],"creatorManageDataTotal":1,"mcnDomains":[]},"commentManage":{"commentList":{"ids":[],"entities":{},"nextOffset":0,"urlToken":""},"subCommentList":{"ids":[],"entities":{},"paging":{"next":"","isEnd":false}}},"zhiPlus":{"permissionStatus":9999},"streaming":{}},"subAppName":"main"}</script><script crossorigin="" src="https://static.zhihu.com/heifetz/vendor.f04deb6fd8cc3116a458.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/main.app.4e608ef494cfe233d4ad.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/main.question-routes.a67f258dc08684958e89.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script></html>